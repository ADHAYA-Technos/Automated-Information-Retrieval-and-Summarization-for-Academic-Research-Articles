{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install fitz\n",
        "\n",
        "# !pip install dotenv chromadb evaluate gradio smolagents\n",
        "\n",
        "# !pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBP6QNJ6O483",
        "outputId": "ce89ca79-3fcb-40b3-c914-61e4012a309c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Using cached pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Using cached pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "Installing collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import fitz\n",
        "import chromadb\n",
        "import numpy as np\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "import uuid\n",
        "\n",
        "# Hugging Face & LangChain for AI models\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from smolagents import OpenAIServerModel, HfApiModel\n",
        "from smolagents.tools import Tool\n",
        "from typing import Dict, List, Union\n",
        "\n",
        "# Google Drive connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85SJ6kdW0_xI",
        "outputId": "c0a92572-8a40-441b-d2cb-23246a12cb61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "os.environ[\"CHROMA_HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB\n",
        "client = chromadb.PersistentClient(path=\"/content/drive/MyDrive/VectorDB\")\n",
        "collection = client.get_or_create_collection(name='ties_collection_emb', metadata={\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "print(collection)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWJP3yCx1FDH",
        "outputId": "d38decdf-ebbc-4d70-b039-49b9e47bec5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection(name=ties_collection_emb)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_scientific_text(text):\n",
        "    \"\"\"Cleans text by removing references, formulas, tables, numerical results, and dataset details.\"\"\"\n",
        "    patterns = [r'\\[\\d+\\]',  # References\n",
        "        r'L[a-zA-Z]+\\s*=\\s*[^=]+',  # Formulas\n",
        "        r'TABLE \\d+\\..*?\\n',  # Table headers\n",
        "        r'\\b\\d+%|\\b\\d+\\.\\d+%|\\b\\d{3,}\\b',  # Numerical results\n",
        "        r'https?://[^\\s]+',  # URLs\n",
        "        r'\\b\\d+\\s*(domains|records|samples|queries)\\b',  # Dataset-specific details\n",
        "        r'^\\w+\\s+\\d+\\s+[^\\s]+\\.[a-z]+$',  # DGA dataset examples\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
        "    return '\\n'.join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "\n",
        "def extract_and_chunk_pdf(file_path, chunk_size=800, chunk_overlap=400):\n",
        "    \"\"\"Extracts and chunks text from a PDF.\"\"\"\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    abstract = text.split(\"\\n\\n\")[0] if \"abstract\" in text.lower() else \"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    chunks = [process_scientific_text(chunk) for chunk in chunks]\n",
        "    return chunks, abstract\n"
      ],
      "metadata": {
        "id": "ptf5qC7w1G2K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_scientific_text(text):\n",
        "    \"\"\"Cleans text by removing references, formulas, tables, numerical results, and dataset details.\"\"\"\n",
        "    patterns = [r'\\[\\d+\\]',  # References\n",
        "        r'L[a-zA-Z]+\\s*=\\s*[^=]+',  # Formulas\n",
        "        r'TABLE \\d+\\..*?\\n',  # Table headers\n",
        "        r'\\b\\d+%|\\b\\d+\\.\\d+%|\\b\\d{3,}\\b',  # Numerical results\n",
        "        r'https?://[^\\s]+',  # URLs\n",
        "        r'\\b\\d+\\s*(domains|records|samples|queries)\\b',  # Dataset-specific details\n",
        "        r'^\\w+\\s+\\d+\\s+[^\\s]+\\.[a-z]+$',  # DGA dataset examples\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
        "    return '\\n'.join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "\n",
        "def extract_and_chunk_pdf(file_path, chunk_size=800, chunk_overlap=400):\n",
        "    \"\"\"Extracts and chunks text from a PDF.\"\"\"\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    abstract = text.split(\"\\n\\n\")[0] if \"abstract\" in text.lower() else \"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    chunks = [process_scientific_text(chunk) for chunk in chunks]\n",
        "    return chunks, abstract\n"
      ],
      "metadata": {
        "id": "Aj6Swljq1Iel"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98TSOlY-2xoT",
        "outputId": "22b6fcf1-7535-4607-899d-e53cf4c4ff28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sections(paper_text):\n",
        "    \"\"\"Extracts structured sections from scientific papers.\"\"\"\n",
        "    patterns = {\n",
        "        \"Abstract\": r\"(?i)\\bAbstract\\b[:\\.\\s]*([\\s\\S]*?)(?=\\n\\s*\\b(?:Introduction|1\\s*Introduction)\\b)\",\n",
        "        \"Introduction\": r\"(?i)\\bIntroduction\\b[:\\.\\s]*([\\s\\S]*?)(?=\\n\\s*\\b(?:Methods|2\\s*Methods)\\b)\",\n",
        "        \"Methods\": r\"(?i)\\b(?:Methods|Methodology)\\b[:\\.\\s]*([\\s\\S]*?)(?=\\n\\s*\\b(?:Results|3\\s*Results)\\b)\",\n",
        "        \"Results\": r\"(?i)\\bResults\\b[:\\.\\s]*([\\s\\S]*?)(?=\\n\\s*\\b(?:Discussion|4\\s*Discussion)\\b)\",\n",
        "        \"Discussion\": r\"(?i)\\bDiscussion\\b[:\\.\\s]*([\\s\\S]*?)(?=\\n\\s*\\b(?:Conclusion|5\\s*Conclusion)\\b)\",\n",
        "        \"Conclusion\": r\"(?i)\\bConclusion\\b[:\\.\\s]*([\\s\\S]*?)(?=\\n\\s*\\b(?:References|6\\s*References|\\Z))\"\n",
        "    }\n",
        "    sections = {key: re.search(pattern, paper_text).group(1).strip() for key, pattern in patterns.items() if re.search(pattern, paper_text)}\n",
        "    return \"\\n\\n\".join([f\"{key}:\\n{value}\" for key, value in sections.items()]) if sections else \"Full Text:\\n\" + paper_text\n",
        "\n",
        "\n",
        "def baseline_model(pdf_path):\n",
        "    chunks, abstract = extract_and_chunk_pdf(pdf_path)  # Extract text chunks\n",
        "    chunks = [process_scientific_text(chunk) for chunk in chunks]\n",
        "\n",
        "    # **Chunk large texts into manageable sizes**\n",
        "    def chunk_text(text, max_length=500):\n",
        "        return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "    chunks = chunk_text(\" \".join(chunks))\n",
        "\n",
        "    # **Summarize each chunk separately with dynamic max_length**\n",
        "    summaries = [\n",
        "        summarizer(\n",
        "            f\"Summarize this: {chunk}\",\n",
        "            max_length=max(50, int(len(chunk.split()) * 0.75)),  # Adjust based on word count\n",
        "            min_length=50,\n",
        "            do_sample=False\n",
        "        ) for chunk in chunks\n",
        "    ]\n",
        "\n",
        "    # **Merge all summaries into a final output**\n",
        "    summary = \" \".join([s[0]['summary_text'] for s in summaries])\n",
        "\n",
        "    clean_response = re.sub(r\"<think>.*?</think>\", \"\", summary, flags=re.DOTALL).strip()\n",
        "    return clean_response\n"
      ],
      "metadata": {
        "id": "380TXyyJ1KE2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary =   baseline_model(\"my_pdf.pdf\")"
      ],
      "metadata": {
        "id": "3JoyX_tU1YxM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(summary))"
      ],
      "metadata": {
        "id": "VuLc0osp1Z6n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "900feab8-3074-47d3-eaad-580c5797961d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The abstract reasoning language model (ALM) has been used to solve many complex logical inference problems. However, it is not always the best choice for all queries. This paper argues that LLM can be used to solve a ny problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? We propose Thinkless, a learnable framework that em- powers an LLM to adaptively select between short-form and long-form reasoning Using a decoupled formulation of GRPO, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response . . . . . . . . . . . . . . . . . . . . . . . . . .  Extend the model to all questions. Avoid using extended reasoning for all questions. Avoid using extended reasoning for all questions. Avoid using extended reasoning for all questions. Avoid using extended reasoning for all questions. Avoid using extended reasoning for all questions. . . . . . . . . . . . . . . . . . . . . . . . . . . . think> short> Auto Decision Obj 1: Mode Sel think> short> Auto Decision Obj 1: Mode Sel think> short> Auto Decision Obj 1: ection Obj 2: Accuracy Improvement Decoupled GRPO Figure 1: Thinkless learns a hybrid LLM capable of adaptively selecting between thinking and non-thinking inference modes, directed by two special tokens, think> and a hybrid reasoning model that can generate short-form or long-form responses based on the input query. The model is guided by the complexity of the query and its own capability. The model is tasked with autonomously deciding whether to generate a short-form or Identify the model’s capabilities. Identify the user’s tolerance for the trade-off between efficiency and accuracy. Identify the model’s ability to learn from interactions. Identify the model’s ability to learn from interactions. a hybrid reasoning model capable of selecting between short-form and long-form responses. Through iterative exploration and reward-driven updates, the model progressively acquires the ability to make autonomous, context-aware decisions about its reasoning Observe the model’s response style. Train the model. Observe the model’s response style. Observe the model’s response style. Observe the model’s response style. Observe the model’s response style. Decouple the model into two models: a reasoning model and a standard instruction-following model. Train the model on paired long- and short-form responses. Decouple the model into two models: a reasoning model . . . . . . . . . . . . . . . . . . . . . . . . . . .  elongation of long & short responses, the single control token may receive weak and biased gradient signals, ultimately leading to mode collapse at the early stages of training. To this end, we propose Thinkless, a reinforcement learning framework designed to train a hybrid reasoning Train the model to respond to control tokens, think> and short>, which are generated as the first token in the model’s output to signal the intended inference style. Warm-up the model. Observe the model’s response g model, each conditioned on a specific control token (think> or short>). Additionally, the model is trained on paired long- form and short- form responses for each query. Reinforcement Learning with De a framework for hybrid reasoning. Understand the problem. Understand the solution. Identify the problem. Identify the solution. Identify the solution. Identify the solution. Identify the solution. Identify the solution. Identify the solution. Identify the solution DeGRPO is a method for hybrid reasoning that explicitly separates the hybrid reasoning objective into two components: Mode Selection, which governs how quickly the policy adapts based on the model’s current accuracy; and Accuracy Improvem, which governs how Adapt the mode selection policy to the response tokens. Balance the learning signals for the control and response tokens. Adapt the mode selection policy to the response tokens. Adapt the mode selection policy to the response tokens. Adapt the mode DeGRPO uses a multi-layer reinforcement learning model. DeGRPO uses a multi-layer reinforcement learning model. DeGRPO uses a multi-layer reinforcement learning model. DeGRPO uses a multi-layer reinforcement learning model. De Decoupled GRPO method for reasoning inference cost reduction. Adaptive reasoning inference cost reduction. Adaptive reasoning inference cost reduction. Adaptive reasoning inference cost reduction. Adaptive reasoning inference cost reduction. g Models. g Models generate intermediate steps in a chain-of-thought process before producing a final answer. To mitigate this, recent research has explored strategies to enhance the efficiency of reasoning models without sacrificing accuracy eliciting a concise reasoning path. Using a variety of techniques, such as reinforcement learning with length penalties [1, 24], supervised fine-tuning using variable-length chain of thought data , and prompt- Improve the efficiency of the model. Optimize the decoding strategy. Optimize the hybrid reasoning strategy. Optimize the decoding strategy. Optimize the hybrid reasoning strategy. Optimize the hybrid reasoning strategy. Optimize the hybrid reasoning strategy. Optimize Hybrid reasoning is a form of reasoning that combines short-chain reasoning with long-chain reasoning. Hybrid reasoning is a form of reasoning that combines short-chain reasoning with long-chain reasoning. Hybrid reasoning is A learning-based approach to unified models that can support both primary and secondary reasoning.  2016 Elsevier B.V. All rights reserved. This work is part of the Elsevier B.V. Open Access a model of reasoning that is based on inputs. a model of reasoning that is based on inputs. a model of reasoning that is based on inputs. a model of reasoning that is based on input Improve model size and decoding strategies. Improve model complexity. Improve model speed. Improve model quality. Improve model performance. Improve model quality. Improve model speed. Improve model quality. Improve model speed. Improve model quality. Improve model speed. Improve model Embrace hybrid reasoning. Use routing mechanisms to ensure that models are not redirected. Use unified models to ensure that models are not redirected. Use routing mechanisms to ensure that models are not redirected. Use routing mechanisms to ensure that LLMs can be trained to support both reasoning modes and can switch between them via prompt-based control. However, most existing approaches depend on manually crafted heuristics to balance efficiency and performance. In this work, we explore a learning-based Hinkless is a framework for generating short- and long-form responses. It is implemented in two stages: (1) Distillation for Warm-up, where we fine- tune a pre-trained reasoning model to unify two reasoning styles, and (2) think> short> think> short> short> think> short> short> think> short> short> think>  Prepare for reinforcement learning with Decoupled GRPO. Prepare for distillation. Prepare for reinforcement learning with Decoupled GRPO. Prepare for reinforcement learning with a hybrid model. Prepare for reinforcement learning with a hybrid model.                                  Ddistill is a model that distills a control token into a multi-style response distribution conditioned on the control token. 3.2 Learning When to Think via Decoupled GRPO is a model that distills a control token A reinforcement-learning approach to mode selection for a given input x. We propose a policy (c, a | x) = (c | x) (a | x, c), where the first token c C =                                   Ddistill is a model that distills the model’s response into two types of responses. 3.2 Learning When to Think via Decoupled GRPO. The model can produce both long- and short-form answers. e is a feature of the model that can be learned to select the mode that best suits the input x. To achieve this, we propose a policy (c, a | x) = (c | x) (a | x, c) where a ai,0 C is the control token. Reward Design. Let ydenote the ground-truth answer corresponding to the input x. We consider a minimally designed reward function r(a, y, c) =   JGRPO() = Ex,aiG  x  i = 1  Ti+1 TiX  t = 0 Li,t()  DKL ( | x) ref( | x) # , where , 1 +  Ai,t (2) We compute the relative advantage using Ai,t = r mean(r) using . This 1.0 , if c = think> and Extract-Answer(a) = d. The objective is defined as: JGRPO() = Ex,ai \" 1 G G X i=1 1 Ti+1 Ti X t=0 Li,t()  DKL ( | x) ref( | x) #, where Li,t( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Identify the control token ai,0. Identify the mode-accuracy imbalance. Identify the imbalance between mode-accuracy and mode-inference. Identify the imbalance between mode-inference and mode-respons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                  nt  is introduced to balance the optimization between mode selection and response generation. Set  = 1/ for stable training. nt  is introduced to balance the optimization between mode selection and response generation. Set  = 1/ for stable training. GRPO-based model for mode selection and response generation. Using token-level advantages to balance the contributions of mode selection and response generation. Using token-level advantages to balance the contributions of mode selection and response generation. Using token-level advantages 4.1 Experimental Setups LLMs and Datasets. DeGRPO () = Ex,ai \" 1 G G X i=1  Li,0() | z  Control Token + 1 Ti Ti X t=1 Li,  = 1 for mode selection and think-short imbalance. Set  = 1 for mode update. Set  = 1 for mode selection and think-short imbalance. Set  = 1 for mode update. Set  = 1 for mode update. GRPO is a general-purpose framework for learning to learn to answer questions. It is based on the GRPO framework and uses the same model as the GRPO framework. The model is based on the GRPO model. ental Setups LLMs and Datasets. We utilize DeepSeek-R1-Distill-Qwen-1.5B as the base model to train a hybrid reasoning policy. To construct long- . . . . . . . . . . . . . . . . . . . . . . . . . . Using the AIME Minerva Algebra and the GSM-8K models, we trained the models on the following datasets: - - - - - - - - - - - - - - - - - - . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . RL experiments were performed using the VeRL framework. The following experiments were performed: CoT-Valve  = 4  = 4  = 4  = 4  = 4  = 4  = 4  = 4  = 4  = 4  = 4  = 4  = 4                                4.2 Empirical Results on Hybrid Reasoning Finding 1. The hybrid reasoning model learned by the method is able to distinguish complex from simple queries, reducing the use of thinking by –. Table 1 presents a comparison between our method and several existing mods. ets or techniques. The first part showcases our baseline model, DeepSeek-R1-Distill-Qwen-1.5B, alongside two instruction-following models designed to generate concise answers. On challenging datasets Consider the performance of the various models. Consider the use of dels. Consider the use of model merging. Consider the use of model merging. Consider the use of model merging. Consider the use of model merging. Consider the use of model . . . . . . . . . . . . . . . . . . . . . . . . . .  . The method is compared to existing models to demonstrate the effectiveness of the method. The results show that the method is able to generate a large number of tokens, compared to existing models. The method is able to generate a large number . . . . . . . . . . . . . . . . . . . . . . . . . . Use supervised fine-tuning techniques. Adjust reasoning length. Adapt reasoning length to the model. Adapt reasoning length to the model. Adapt reasoning length to the model. Adapt reasoning length to the model. Adapt reasoning length to the We show that the proposed heuristics are effective on a range of datasets, but that they may not be optimal on all datasets. We also show that hybrid reasoning strategies can be used to improve the performance of a model. iii. iiii. iv. v. v. v. v. v. v. v. v. v. v. v. v. v Adaptive reasoning is a powerful method for reasoning about the complexity of the input. It is a novel approach to reasoning about the complexity of the input. It is a novel approach to reasoning about the complexity of the input. It is a novel RLs can be used to train a model to learn to recognize short and long responses. The RLs can be used to train a model to learn to recognize short and long responses. The RLs can be used to train a model to learn to recognize short and long responses. b) The proposed Decoupled GRPO, with a U-shape learning curve. AIME Math Minerva Algebra GSM8k 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of queries P@1= P@1= P@1= P@1= d) Learned Policy of DeGRPO. d) Learning Dynamics in RL Finding 2. Policy may collapse due to imbalanced update of control tokens in Vanilla GRPO. Mode Collapse in RL. To further analyze how the model learns a reasonable policy te the Mode Collapse problem in standard GRPO. In conventional GRPO, the gradient on the control token is normalized by the total length of the response, which introduces an imbalance between long and short outputs. . Decouple the GRPO model from the short mode. . Decouple the GRPO model from the short mode. . Decouple the GRPO model from the short mode. . Decouple the GRPO model from the short mode. 4.3 Training Dynamics in RL Finding 2. Policy may collapse due to imbalanced update of control tokens in Vanilla GRPO. Mode Collapse in RL. To further analyze how the model learns a reasonable policy, we visualize the training process of RL. In conventional GRPO b, the model collapses quickly, as the number of generated long-chain responses drops below 10 within just update steps. c, the model collapses quickly, as the number of generated short-chain responses drops below 10 within just update steps. Identify the problem of GRPO collapse. Decouple the GRPO algorithm from the model. Identify the problem of GRPO collapse. Decouple the GRPO algorithm from the model. Identify the problem of GRPO collapse. RL model shows preference for long-chain reasoning in the early stages of training. As training progresses, we observe an improvement in the accuracy of short-chain responses. RL model shows preference for long-chain reasoning in the early stages of training. Increase the number of short-chain responses. Increase the number of short-chain responses. Increase the number of short-chain responses. Increase the number of short-chain responses. Increase the number of short-chain responses. Increase the number of short-chain responses ly, we observe a decline 7 Model Mode & Teacher AIME Minerva Algebra Math- GSM8K Pass@1 #Tokens Pass@1 #Tokens Pass@1 #Tokens Pass@1 #Tokens Pass@1 # Use the following SFT datasets: think> 0. short> 0. short> 0. short> 0. short> 0. short> 0. short> 0. short> 0. short . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Long (R1-671B) 0 Long (QMath-1.5B) 0 Short (QMath-1.5B) 0 Long (R1-671B) 0 Short ( Train the model on a variety of tasks. Observe a decrease in the accuracy of long-chain responses during the latter half of training. Find the weight of the control token. 0 Training Step 0 5 10 15 20 25 Visualize the effect of decoupling on model behavior.                         Observe the effect of the weight on the control token on the short-response accuracy. Observe the effect of the weight on the control token on the short-response accuracy. Observe the effect of the weight on the control token Using knowledge distillation, we find that LLMs can be a good short response learner. In this work, knowledge distillation is deployed for warm-up, serv- ing, and re- sults. . We present a detailed implementation of the model, including a comparison of three datasets. We present a detailed analysis of the model's architecture, including a comparison of the model's architecture to the DeepSeek- a large collection of datasets; and (3) OpenThoughts-1M, a large-scale and diverse collection that subsumes the former two. We observe that generating short responses trajectories. We compare the results under two settings: a Use a fast policy update to improve the model’s performance. Avoid using too many policy updates. Avoid using too many policy updates. Avoid using too many policy updates. Avoid using too many policy updates. Avoid using too many policy updates. n initial accuracy rather than a collaborative learning of mode selection and accuracy improvement. 4.4 Details of Warm-up Distillation Finding 5. Reasoning LLMs can be a good short response learner. In this work, knowledge distillation is deployed for warm- a multi-domain dataset labeled by DeepSeek-R1-67B. We find that generating short responses using a long-chain approach is more effective than generating short responses using a short-chain approach. Obtain a distillation model. Obtain a distillation model. Obtain a distillation model. Obtain a distillation model. Obtain a distillation model. Obtain a distillation model. Obtain a distillation Find the probability of think> on MA. Find the projection of mathbfa$ onto mathbfb. Find the area of the graph of mathbfb. Select the LLM response. Analyze the results. Understand the limitations of LLM. Analyze the results of the LLM. Understand the limitations of LLM. Analyze the results of the LLM. Understand the limitations of LLM. lts in only a improvement in long-chain accuracy on the Math- benchmark. This work provides a preliminary validation of the effectiveness of simple distillation, and we leave the construction of stronger initial hybrid models as an important direction of future research. ereys of different difficulty. In addition, we highlight representative examples corresponding to high, medium, and low confidence levels to illustrate the model’s decision behavior. We observe that samples assigned to the short reasoning mode are typically simple arithm ibrated policy that adapts reasoning depth based on task complexity. 5 Limitations and Future Works This work presents an effective reinforcement learning framework that enables a hybrid model to adapt its inference mode based on both problem complexity and its own s in a slight performance drop in the initial model for reinforcement learning. Exploring better strategies for constructing the hybrid model, such as merging techniques or lightweight fine- tuning methods like LoRA to mitigate catastrophic forgetting. s in a slight omains to enable more general and practical hybrid reasoning capabilities. 6 Conclusion This paper proposes a reinforcement learning framework for building a hybrid reasoning model. It autonomously decides whether to generate a short response or engage in long-form reasoning based on the . . The model is well-calibrated and can be used to model a variety of questions of different difficulty. The model can be used to model the decision behavior of a given question. The model can be used to model the decision behavior a hybrid model that adapts its inference mode based on problem complexity. 5 Limitations and Future Works This work presents an effective reinforcement learning framework that enables a hybrid model to adapt its inference mode based on both problem complexity and its own ht performance drop in the initial model for reinforcement learning. Exploring better strategies for constructing the hybrid model, such as merging techniques or lightweight fine- tuning methods like LoRA to mitigate catastrophic forgetting. a broader range of datasets . nable more general and practical hybrid reasoning capabilities. 6 Conclusion This paper proposes a reinforcement learning framework for building a hybrid reasoning model. It autonomously decides whether to generate a short response or engage in long-form reasoning based on Decouple the long-form reasoning model from the short-form reasoning model. Decouple the short-form reasoning model from the long-form reasoning model. Decouple the short-form reasoning model from the long-form reasoning model hought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:., . Akhiad Bercovich, Itay Levy, The following are the names of the finalists for the 2019 World Championship of Athletics in Athletics: d, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia The winners of the ICC World Youth Championships in Dubai will be announced on Thursday. The winners will be announced on Friday. The ICC World Youth Championships in Dubai will be held from 10 to 14 November. The ICC World Youth Championship The winners of the prestigious IFBB World Women's Championships in Dubai have been announced. The winners of the IFBB World Women's Championships in Dubai have been announced. The winners of the IFBB World Women The following players have been selected for the 2018 Indian Super League: - Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Je The list of the finalists of the FIVB Women's World Championships in Athletics in Brazil includes:              ya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Ol Adaptive Thinking for Reasoning in Large Language Models. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qi Research on learning to reason. w. - - - - - - - - - - - - - - - - - - - - -  iii. Search for llms via reinforcement learning. arXiv preprint arXiv:., . Xiaoshu Chen, Sihang Zhou, Ke Liang, and Xinwang Liu. Read the papers. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read the reviews. Read Open r1: A fully open reproduction of deepseek-r1, January. Open r1: A fully open reproduction of deepseek-r1, January. Open r1: A fully open reproduction of deepseek-r1, January. Token-budget- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Research on efficient reasoning models. ArXiv preprint arXiv:., . Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:., . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . .  Research the effects of distillation on reasoning in small languages. Investigate the effects of distillation on reasoning in large models. Investigate the effects of distillation on reasoning in large models. Identify the effects of distillation on reasoning in large models. Research on r1-zero-like training. Research on r1-zero-like training. Research on r1-zero-like training. Research on r1-zero-like training. Research on r1-zer . Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Surpassing o1-preview with a 1.5 b model by gth- compressible chain-of-thought tuning. arXiv preprint arXiv:., . Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and . Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Deepscaler: Surpassing o1-preview with a 1.5 b model by ght tuning. arXiv preprint arXiv:., . Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. ent thoughts: On the power of looped transformers. . Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowe . . . . . . . . . . . . . . . . . . . . . . . . . . Read the following papers: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Research on reasoning in large language models. Research on reasoning in large language models. Research on reasoning in large language models. Research on reasoning in large language models. Research on reasoning in large language models. Research on reasoning in large language models. Research on reasoning in large language models. Research ms with tools for the deep research. arXiv preprint arXiv:., . 11 Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing Research on simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. ArXiv preprint arXiv:., Xunyu Zhu"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(texts):\n",
        "    \"\"\"Compute embeddings for a list of texts using the SentenceTransformer model.\"\"\"\n",
        "    return embedding_model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "def retrieve_similar_chunks(chunks, embeddings, query, top_k=5):\n",
        "    \"\"\"Retrieve top_k most relevant chunks to the query using cosine similarity.\"\"\"\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "    similarities = np.inner(query_embedding, embeddings)  # Cosine similarity\n",
        "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    return [chunks[i] for i in top_k_indices]\n",
        "\n",
        "def classical_rag(pdf_path, user_query, top_k=5):\n",
        "    \"\"\"Performs a simple RAG-like summarization using embeddings + summarizer.\"\"\"\n",
        "    # Step 1: Extract and clean PDF content\n",
        "    pdf_chunks, _ = extract_and_chunk_pdf(pdf_path)\n",
        "    cleaned_chunks = [process_scientific_text(chunk) for chunk in pdf_chunks]\n",
        "\n",
        "    # Step 2: Embed chunks\n",
        "    embeddings = compute_embeddings(cleaned_chunks)\n",
        "\n",
        "    # Step 3: Retrieve most relevant chunks to the query\n",
        "    relevant_chunks = retrieve_similar_chunks(cleaned_chunks, embeddings, user_query, top_k=top_k)\n",
        "\n",
        "    # Step 4: Create prompt and summarize\n",
        "    context = \"\\n\".join(relevant_chunks)\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following content in response to the question:\n",
        "\"{user_query}\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "    summary = summarizer(prompt, max_length=200, min_length=60, do_sample=False)\n",
        "    return summary[0]['summary_text']\n"
      ],
      "metadata": {
        "id": "51SrauYK9Pt3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_summary = classical_rag(\"my_pdf.pdf\", \"What are the main contributions of the paper?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yTkLvvv9-cQ",
        "outputId": "c9c26d62-cb0c-4d4f-e341-7816c1583ba5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-bef375832800>:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
            "  similarities = np.inner(query_embedding, embeddings)  # Cosine similarity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(rag_summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "ds7g796A-Rho",
        "outputId": "3fa36ef3-2da8-416d-a147-9293fe4e0900"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper presents a survey of the state-of-the-art in the field of machine learning and reasoning. It presents a survey of the state-of-the-art in the field of machine learning and reasoning. It presents a survey of the state-of-the-art in the field of machine learning and reasoning."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def rag_summary(query, k=5):\n",
        "#     results = collection.query(query_texts=[query], n_results=k)\n",
        "#     context = \" \".join([doc for doc in results['documents'][0]])\n",
        "\n",
        "#     summary = summarizer(\n",
        "#         f\"Summarize based on context: {context}\",\n",
        "#         max_length=200,\n",
        "#         min_length=50,\n",
        "#         do_sample=False\n",
        "#     )\n",
        "#     return summary[0][\"summary_text\"]\n"
      ],
      "metadata": {
        "id": "zLRT0z0zizYS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize agentic model\n",
        "# agent_model = HfApiModel(name=\"google/flan-t5-large\")\n",
        "\n",
        "# agentic_tools = [\n",
        "#     Tool(name=\"lookup_vector\", func=lambda q: \" \".join(collection.query(query_texts=[q], n_results=5)['documents'][0]), description=\"Retrieve context\")\n",
        "# ]\n",
        "\n",
        "# def agentic_rag_summary(query):\n",
        "#     response = agent_model.run_task(\n",
        "#         system_prompt=\"You are a scientific summarizer using external tools.\",\n",
        "#         task_prompt=f\"Summarize this query: {query}\",\n",
        "#         tools=agentic_tools\n",
        "#     )\n",
        "#     return response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8JlsnPTi345",
        "outputId": "43adcc5f-a625-4844-e3a0-106ef3712ea3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/smolagents/models.py:1325: FutureWarning: HfApiModel was renamed to InferenceClientModel in version 1.14.0 and will be removed in 1.17.0.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def agentic_rag_summary(query):\n",
        "    # Step 1: Retrieve relevant chunks from ChromaDB\n",
        "    retrieved = collection.query(query_texts=[query], n_results=5)\n",
        "    context = \" \".join(retrieved['documents'][0])\n",
        "\n",
        "    # Step 2: Prompt model to summarize based on query + context\n",
        "    prompt = f\"Given the following context, answer the question.\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
        "\n",
        "    result = summarizer(prompt, max_length=300, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "v6kxTY1U0wdb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentic = agentic_rag_summary(\"Summarize the scientific paper in the PDF.\")"
      ],
      "metadata": {
        "id": "Ph_rMYae_ptI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(agentic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "nrPUdXaX_zGy",
        "outputId": "0049dea5-e107-4349-91bd-166e9cd577aa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The authors report the results of a study on the training of a hybrid model for reinforcement learning. The model was trained on the Megatron framework. The model was trained on the Megatron framework. The model was trained on the Megatron framework."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "# Load evaluation metrics\n",
        "bleu = load(\"bleu\")\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Get summaries\n",
        "baseline = summary\n",
        "rag = rag_summary\n",
        "agentic = agentic\n",
        "\n",
        "# Use original abstract or gold summary as reference\n",
        "_, reference = extract_and_chunk_pdf(\"my_pdf.pdf\")\n",
        "references = [reference]\n",
        "\n",
        "# Format predictions\n",
        "baseline_pred = [baseline]\n",
        "rag_pred = [rag]\n",
        "agentic_pred = [agentic]\n",
        "\n",
        "# Evaluate\n",
        "print(\"== BLEU Scores ==\")\n",
        "print(\"Baseline:\", bleu.compute(predictions=baseline_pred, references=references))\n",
        "print(\"RAG:\", bleu.compute(predictions=rag_pred, references=references))\n",
        "print(\"Agentic RAG:\", bleu.compute(predictions=agentic_pred, references=references))\n",
        "\n",
        "print(\"\\n== ROUGE Scores ==\")\n",
        "print(\"Baseline:\", rouge.compute(predictions=baseline_pred, references=references))\n",
        "print(\"RAG:\", rouge.compute(predictions=rag_pred, references=references))\n",
        "print(\"Agentic RAG:\", rouge.compute(predictions=agentic_pred, references=references))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVU9pScAi4wg",
        "outputId": "7148d663-9a80-45d1-81a8-811105f69131"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== BLEU Scores ==\n",
            "Baseline: {'bleu': 0.028994708470058906, 'precisions': [0.08644032493230577, 0.03395833333333333, 0.01833715357366118, 0.013130471029595666], 'brevity_penalty': 1.0, 'length_ratio': 7.896381578947368, 'translation_length': 4801, 'reference_length': 608}\n",
            "RAG: {'bleu': 0.0, 'precisions': [0.6122448979591837, 0.08333333333333333, 0.0, 0.0], 'brevity_penalty': 1.11044651440178e-05, 'length_ratio': 0.0805921052631579, 'translation_length': 49, 'reference_length': 608}\n",
            "Agentic RAG: {'bleu': 0.0, 'precisions': [0.5869565217391305, 0.044444444444444446, 0.0, 0.0], 'brevity_penalty': 4.943725781036531e-06, 'length_ratio': 0.0756578947368421, 'translation_length': 46, 'reference_length': 608}\n",
            "\n",
            "== ROUGE Scores ==\n",
            "Baseline: {'rouge1': np.float64(0.16701853155055127), 'rouge2': np.float64(0.0699366345928186), 'rougeL': np.float64(0.08069434670419892), 'rougeLsum': np.float64(0.1482524044100399)}\n",
            "RAG: {'rouge1': np.float64(0.12099644128113879), 'rouge2': np.float64(0.007142857142857142), 'rougeL': np.float64(0.08896797153024912), 'rougeLsum': np.float64(0.12099644128113879)}\n",
            "Agentic RAG: {'rouge1': np.float64(0.10200364298724955), 'rouge2': np.float64(0.010968921389396709), 'rougeL': np.float64(0.0546448087431694), 'rougeLsum': np.float64(0.10200364298724955)}\n"
          ]
        }
      ]
    }
  ]
}