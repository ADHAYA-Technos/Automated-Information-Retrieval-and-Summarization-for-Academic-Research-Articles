A Framework for Fine-Tuning LLMs using Heterogeneous Feedback
Ryan Aponte 1, Ryan A. Rossi 2, Shunan Guo2, Franck Dernoncourt2,
Tong Yu2, Xiang Chen2, Subrata Mitra2, Nedim Lipka 2
1Carnegie Mellon University, 2Adobe Research
Abstract
Large language models (LLMs) have been
applied to a wide range of tasks, including
text summarization, web navigation, and chat-
bots. They have benefitted from supervised
fine-tuning (SFT) and reinforcement learning
from human feedback (RLHF) following an un-
supervised pretraining. These datasets can be
difficult to collect, limited in scope, and vary in
sample quality. Additionally, datasets can vary
extensively in supervision format, from numer-
ical to binary as well as multi-dimensional with
many different values. We present a framework
for fine-tuning LLMs using heterogeneous feed-
back, which has two main components. First,
we combine the heterogeneous feedback data
into a single supervision format, compatible
with methods like SFT and RLHF. Next, given
this unified feedback dataset, we extract a high-
quality and diverse subset to obtain perfor-
mance increases potentially exceeding the full
dataset.
We conduct extensive experiments
to understand the effectiveness of these tech-
niques for incorporating heterogeneous feed-
back, and demonstrate improvements from us-
ing a high-quality and diverse subset of the data.
We find that our framework is able to improve
models in multiple areas simultaneously, such
as in instruction following and bias reduction.
Introduction
LLMs are fine-tuned for a variety of purposes,
such as for instruction following in Instruct-
GPT (Ouyang et al., 2022). The fine-tuning process
generally begins with collecting examples of de-
sired model behavior and performing supervised
learning. Some models stop at SFT (Chiang et al.,
2023), while InstructGPT follows this by training
a reward model based on binary human prefer-
ence data. The fine-tuned model is then further
refined using RLHF, using a signal from the re-
ward model. In the example of InstructGPT, the
algorithm used is Proximal Policy Optimization
(PPO) (Schulman et al., 2017). For each of these
steps, the fine-tuning dataset uses a single form of
supervision. Fine-tuning datasets exist for a vari-
ety of purposes, from training chat-based assistants
in OASST (Kopf et al., 2023), coreference resolu-
tion in WinoGrande (Sakaguchi et al., 2019), help-
fulness, honesty, and harmlessness in Anthropic
HHH (Nakano et al., 2021), and logical reasoning
in OpenPlatypus (Lee et al., 2024). Supervision
format varies, from binary preference in Anthropic
HHH, to several numerical labels OASST, to a
string response in OpenPlatypus. Although fine-
tuning has been successful in mitigating the limi-
tations of pretrained LLMs, these methods require
data of a single supervision type, restricting the
scope of preference data. Recent work has filtered
fine-tuning datasets to reduce cost and increase
quality (Wang et al., 2024). (Wu et al., 2023) use
LLMs to generate embeddings for fine-tuning data
which is clustered with k-center-greedy (Sener and
Savarese, 2018) using an iterative process. (Kung
et al., 2023) randomly delete words in prompts and
measure how the response probability changes as
a measure of the model's uncertainty. (Li et al.,
2024) outperform Alpaca as evaluated by LLM
preference using only 5% of its fine-tuning data.
We present a framework to use multiple fine-
tuning data types, permitting the use of more fine-
tuning datasets and fine-tuning for multiple tasks
simultaneously. Using multiple datasets enables
fine-tuning for different goals simultaneously, such
as for logical reasoning and to reduce bias, and
provides a more accurate view of human preference
by broadening the scope of fine-tuning data. Our
framework selects a high-quality and diverse subset
of the data to make fine-tuning more effective.
Framework
The primary contribution of our framework is to
be able to use fine-tuning data of heterogeneous su-
arXiv:2408.02861v1  [cs.CL]  5 Aug 2024
Figure 1: Framework. First, we concatenate the datasets into a dataset of heterogeneous feedback. We then score
samples based on quality and prompt diversity, remove a fraction of the samples (a hyperparameter), forming the
homogeneous dataset Dtrain. Standard fine-tuning methods are then applied to a pre-trained LLM.
pervision. Figure 1 includes a high-level overview.
Our framework utilizes the simplest supervision,
such as binary preference, and projects all remain-
ing datasets into that format. Because some data
may be redundant in the unified dataset, we filter
for quality and diversity to generate Dtrain. For
simplicity, we use this dataset for both the SFT and
RLHF steps of fine-tuning, however this is not a
requirement. This generates an LLM fine-tuned
with high-quality and diverse data, LLaMA-HD.
Primary fine-tuning dataset
Given a dataset D of prompts with two responses
using binary preference,
D = {(Pi, Ai,0, Ai,1}M
where P is the prompt, Ai,0 and Ai,1 are answers
to the prompt, with Ai,0 defined as the preferred
response to the prompt. This type of dataset takes
the form of binary preference due to two example
responses to a single prompt. Examples here do not
convey a sense of quality, thus prohibiting ranking.
Secondary fine-tuning dataset
Given a dataset D*of user-specific prompts and
responses (question-answer tuples):
D*= {(Pi, Ai, yi)}N
where Pi and Ai are the ith prompt and response
pair, respectively, and yi Rk is the real-valued
vector denoting the score of various labels for that
pair. For a dataset of this type to be compatible with
our method, it is necessary that there are multiple
responses to the same prompt. For example,
(Pi, Ai', yi') D*
can be the second response to the prompt. This pro-
cess can be repeated for arbitrarily many datasets.
A general method for one axis of supervision is
included in Appendix A.2.
Simple Unionization for Feedback
We take D*and create a dictionary with prompt
as key and responses as a list of all responses to
that prompt. This requires at least two responses
for each prompt to be considered. We can conduct
quality and diversity filtering on these prompts, and
then select the preferred response pairs. Once we
have tuples containing a prompt, preferred, and
non-preferred response, our data from D*are now
in the same format as D, so we take the union.
Quality Selection
We infer example quality based on the numerical la-
bels of responses. For datasets with multiple numer-
ical labels, selection of the label is a hyperparame-
ter likely motivated by the purpose of fine-tuning.
For example, our experiment uses toxicity as this
is related to our objective of reducing bias. For
prompts with more than two responses, the highest
quality pair of responses are those that vary most
in the numerical label. Intuitively, these should
give a strong signal to a reward model because one
response is strongly preferred. Finally, we can rank
and select prompts by the preference difference of
their responses.
Diversity Selection
We select for prompt diversity by generating em-
beddings for each prompt, followed by clustering.
Prompts with similar meaning can be considered
redundant. We follow OpenPlatypus in using a sen-
tence transformer to generate semantic embeddings
to filter datasets (Lee et al., 2024). Embeddings
are then clustered using unsupervised methods like
k-means. We select the top fraction of responses
from each cluster. Both the number of clusters and
fraction of each cluster are hyperparameters.
Training
We use the training pipeline from StackL-
LaMA (Beeching et al., 2023), which uses LLaMA-
7B (Touvron et al., 2023). First, we perform SFT
on the base model. We then train a reward model
using the fine-tuned model. This is followed by
RLHF on the fine-tuned model using PPO. Low-
Rank Adaptation is used to reduce memory usage
and increase parallelization (Hu et al., 2021). We
select varying fractions of the training dataset, as
well as omit filtering, to measure its influence.
Experimental Setup
Heterogeneous Datasets
We use three datasets for our experiments: Wino-
Grande (Sakaguchi et al., 2019) (our primary
dataset), OpenAssistant OASST (Kopf et al., 2023)
(our secondary dataset), and WinoGender (Zhao
et al., 2018) for testing the generalization of our
method. WinoGrande is a coreference resolution
dataset developed as a more challenging alterna-
tive to the Winograd Schema Challenge (Levesque
et al., 2012), as machine learning models exceeded
90% accuracy on the dataset. WinoGrande has
been found to transfer to other wino-style schema
challenges, including WinoGender. OASST is a
conversation dataset consisting of over 10,000 con-
versation trees. This dataset has numerical super-
vision, providing an inherent measure of quality.
WinoGender is a dataset testing gender bias in
coreference resolution that involve a pair of sen-
tences, one conforming to American gender biases
and one against them. Differences in response indi-
cate gender bias.
We fine-tune using either WinoGrande alone
with LLaMA-SFT and LLaMA-RLHF, or with a
combination of WinoGrande and OASST using our
framework. We fine-tune using several subsets of
the data, in addition to the dataset without filtering.
For SFT and training the reward model, we treat
the pro-bias examples of WinoGender as negative
and the anti-bias examples as positive. This follows
from the intuition that language models learn hu-
man biases, so reductions in bias can be achieved
by training models in the opposite direction.
Dataset Filtering
We use the numerical score toxicity in OASST to
measure prompt quality. Prompts are ranked based
on the difference in toxicity between responses.
By reducing toxicity, we may also be able to re-
duce gender bias. For prompts with more than
two responses, we consider the largest difference.
As WinoGrande does not have ordered scoring,
these prompts are not filtered. The number fol-
lowing model name indicates the fraction of the
dataset used, with 1.0 indicating no filtering was
performed. Experimental details about quality and
diversity selection are included in Appendix A.3.
Baselines
We compare our approach that learns from hetero-
geneous human feedback datasets to the following
fundamental baselines: Pre-trained LLM (base),
Pre-trained LLM with SFT using WinoGrande, and
Pre-trained LLM with SFT and RLHF using Wino-
Grande. Our method uses the same heterogeneous
dataset for SFT and RLHF. Eight Nvidia A100
GPUs are used for each step of the process. We
test using LLaMA-7B, however our framework is
naturally able to leverage any other state-of-the-art
large language model. The hyperparameters and
LoRA configuration are included in Appendix A.3.
Metrics
We use several metrics to measure change in gen-
der bias, reported in Table 1. Our metrics use
prompts based on the multiple choice format used
in PaLM (Chowdhery et al., 2022), and are in-
cluded in Appendix A.4. Bias takes the difference
in log probabilities for the correct token in Wino-
Bias for the pro-bias and anti-bias sentences. A
model reflecting no gender bias would have a dif-
ference of 0. Bias (Cluster) performs the same
computation, except it considers the log probabili-
ties for every word in the coreference cluster. This
includes the pronoun used in the Bias metric, so its
values are larger. Bias (Entropy) takes the relative
entropy of the next token logits for the pro-bias and
anti-bias sentences. This measures how different
the model state is as a result of each prompt. An
unbiased model would have a relative entropy of 0.
Accuracy is computed in a generative context,
where the model is asked to complete a sentence.
Generation is stopped after 10 new tokens or a
Table 1: Quantitative Results. Bolded entries denote highest performance. -S indicates model was fine-tuned with
SFT only, -R is SFT followed by RLHF. No filtering was performed on fine-tuning datasets for LLaMA-S and
LLaMA-R.
Model
Bias |
Bias (Entropy) |
Bias (Cluster) |
Accuracy |
Similarity |
LLaMA-Base
0.4585
0.0010
3.0393
0.9482
0.9482
LLaMA-S
1.1721
0.1553
10.3180
0.5953
0.6553
LLaMA-R
0.9247
0.0098
4.2139
0.9457
0.9457
LLaMA-HD-0.2-S
0.4436
0.0580
4.5856
0.9204
0.9204
LLaMA-HD-0.4-S
0.7798
0.0548
6.3741
0.8788
0.9394
LLaMA-HD-0.6-S
0.7947
0.0407
7.5564
0.8327
0.8927
LLaMA-HD-1.0-S
0.4117
0.0333
3.8851
0.9533
0.9533
LLaMA-HD-0.2-R
0.4330
0.0580
3.0892
0.9482
0.9482
LLaMA-HD-0.4-R
0.4287
0.0548
2.9852
0.9602
0.9508
LLaMA-HD-0.6-R
0.4727
0.0010
2.9472
0.9646
0.9571
LLaMA-HD-1.0-R
0.3629
0.0068
3.1570
0.9583
0.9583
punctuation token, whichever is sooner. Accuracy
is averaged over both the pro-bias and anti-bias
prompts, so this is more a measure of utility. We
complement this metric with Similarity, which uses
the same generation. It is how often the result,
either correct or incorrect, for a pair of WinoBias
sentences is shared.
Additionally, we conduct a qualitative experi-
ment. We ask the model to respond to several sim-
ple prompts. Evaluating models in a chatbot-like
context gives another perspective on utility. The
models are given single-sentence prompts such as:
"Give me a list of top vacation
destinations.\n"
Results
Quantitative Results
Quantitative results are reported in Table 1. Re-
sults are rounded to 4 digits after the decimal place.
We find that our method is able to reduce bias by
several metrics relative to all baselines, including
a pre-trained model, while maintaining utility as
measured by accuracy. We also see that using SFT
and RLHF with our framework generally leads to
lower bias than with SFT only. Based on the re-
sults for Bias (Entropy), Bias (Cluster), and Ac-
curacy, we can get higher performance by filter-
ing for data quality and diversity than with using
the full fine-tuning dataset. We believe this re-
sult may be improved by examining more rigor-
ous methods for measuring quality and diversity.
The qualitative results also show that our frame-
work permits the improvment on multiple mea-
sures, which are not necessarily correlated, simul-
taneously. With LLaMA-HD-0.6-R and LLaMA-
HD-1.0-R, we achieve higher generative accuracy,
a measure of utility, and higher generative similar-
ity, a measure of bias, relative to the base model.
Qualitative Results
We find that the base and fine-tuned methods us-
ing WinoGrande consistently fail to follow the
prompt. In many instances, the prompt is repeated
indefinitely. With our method, we receive reason-
able responses, likely as a result of our secondary
fine-tuning dataset, OASST, including instruction-
following examples. The qualitative task shows us
that the method is able to train for multiple tasks
at once, namely a reduction in bias and instruc-
tion following. An illustrative sample is included
in Appendix A.5. We observe that while the base
model rarely answers the prompt, LLaMA-S does
on occasion respond reasonably, even though it was
not explicitly instruction fine-tuned. Using only
20% of the filtered dataset, we are able to achieve
consistent instruction following (Appendix A.5).
The highest generative accuracy and lowest bias
(entropy) was also obtained by a model using a
fitlered dataset, demonstrating that filtering can si-
multaneously improve quality and reduce bias.
Conclusion
We find that combining datasets of heterogeneous
supervision for fine-tuning can lead to performance
increases beyond using only one dataset, even when
the secondary dataset is less directly related to the
task. We find that by varying the fraction of used
data, we are able to achieve performance compa-
rable to the full dataset, and sometimes exceed it.
Most significantly, when the reduced bias result
of the quantitative result are combined with the
instruction-following seen in the qualitative result,
we show that it is possible to fine-tune for multiple
purposes simultaneously, even when the datasets
include a different supervision format. Our frame-
work can be used to improve both performance-
oriented metrics, like instruction following, and
to unwanted behavior like bias. This shows that
it is possible to effectively fine-tune LLMs using
heterogeneous supervision.
References
Edward Beeching, Younes Belkada, Kashif Rasul,
Lewis Tunstall, Leandro von Werra, Nazneen Ra-
jani, and Nathan Lambert. 2023. Stackllama: An rl
fine-tuned llama model for stack exchange question
and answering.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021.
Lora: Low-rank adap-
tation of large language models.
arXiv preprint
arXiv:2106.09685.
Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and
Nanyun Peng. 2023.
Active instruction tuning:
Improving cross-task generalization by training on
prompt sensitive tasks. Preprint, arXiv:2311.00288.
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte,
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver
Stanley, Richard Nagyfi, Shahul ES, Sameer Suri,
David Glushkov, Arnav Dantuluri, Andrew Maguire,
Christoph Schuhmann, Huu Nguyen, and Alexander
Mattick. 2023. Openassistant conversations - democ-
ratizing large language model alignment. Preprint,
arXiv:2304.07327.
Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2024.
Platypus: Quick, cheap, and powerful refinement of
llms. Preprint, arXiv:2308.07317.
Hector Levesque, Ernest Davis, and Leora Morgenstern.
2012. The winograd schema challenge. In Thir-
teenth international conference on the principles of
knowledge representation and reasoning.
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang
Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and
Jing Xiao. 2024. From quantity to quality: Boosting
llm performance with self-guided data selection for
instruction tuning. Preprint, arXiv:2308.12032.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger, Kevin Button, Matthew Knight, Benjamin
Chess, and John Schulman. 2021. Webgpt: Browser-
assisted question-answering with human feedback.
CoRR, abs/2112.09332.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. Preprint, arXiv:2203.02155.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2019. Winogrande: An adver-
sarial winograd schema challenge at scale. Preprint,
arXiv:1907.10641.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017.
Prox-
imal policy optimization algorithms.
Preprint,
arXiv:1707.06347.
Ozan Sener and Silvio Savarese. 2018. Active learn-
ing for convolutional neural networks: A core-set
approach. Preprint, arXiv:1708.00489.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothee Lacroix,
Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. Preprint,
arXiv:2302.13971.
Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun
Zhang, and Dianhui Chu. 2024.
A survey on
data selection for llm instruction tuning. Preprint,
arXiv:2402.05123.
Shengguang Wu, Keming Lu, Benfeng Xu, Junyang
Lin, Qi Su, and Chang Zhou. 2023. Self-evolved
diverse data sampling for efficient instruction tuning.
Preprint, arXiv:2311.08182.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018. Gender bias in
coreference resolution: Evaluation and debiasing
methods. Preprint, arXiv:1804.06876.
Appendix
Heterogeneous Dataset Creation
In this section, we include an example of heteroge-
neous dataset creation. The following example is
from WinoGrande, our primary fine-tuning dataset.
It format follows that of Section 2.1, where Ai,0 is
the correct response.
'The box was still visible
after
James
tried
best
manage the wrapper on it. James
should have used the _ that is
small.'
Ai,0: 'box'
Ai,1: 'wrapper'
The following is from OASST, our secondary
fine-tuning dataset. It contains a varying number
of responses to each prompt, as well as numerical
labels. In the experiment, we use the score toxicity.
For brevity, we only include part of the score for
the first response, but each response has several
numerical scores.
Pi': 'Which affordable GPU would
you recommend to train a language
model?'
Ai',0: 'It heavily depends on the
size...'
Ai',1: 'It is difficult to say...'
Ai',2: ...
yi,0:
'toxicity'=
0.00038284,
'spam' = 0, ...
To convert the data of type OASST into that of
our primary dataset, we need to first choose two of
the responses. If the prompt had only one response,
it would be discarded. We select the most and least
toxic responses assocaited with this prompt, and
give the prompt a quality score based on that dif-
ference. For this example, we will consider this to
be responses 0 and 1, with 1 the preferred response.
After filtering for diversity and quality, the data en-
try for the unified dataset would have the following
format:
Pi'': 'Which affordable GPU would
you recommend to train a language
model?'
Ai'',0: 'It is diffucult to say...'
Ai'',1: 'It heavily depends on the
size...'
Now that the format is identical to that of our
primary dataset, we take the union of the primary
and secondary datasets to form the homogenized
dataset. The same process is applied to any addi-
tional datasets. If separate datasets were used for
SFT and RLHF, the process would be repeated.
Converting Supervision
In general, preference datasets can be supervised in
one of three types, in order of increasing complex-
ity: binary, ordinal, and numerical. As projecting
a simpler supervision type can be noisy, we focus
on simplifying supervision. In general, this is the
recipe for converting a single axis of supervision:
* numerical -sort increasing or decreasing,
based on application -ordinal
* ordinal -extract best and worst responses to
prompt -binary
Hyperparameters
We include the hyperparameters and LoRA config-
uration in this section.
Quality and Diversity Selection: We use all-
MiniLM-L6-v2, a sentence transformer designed
to capture semantic information, to generate em-
beddings for each prompt (Reimers and Gurevych,
2019). The data are then separated into 10 clusters
with 10 restarts using k-means. We select the top
20%, 40%, and 60% of prompts from each cluster,
as well as use the full unfiltered dataset for LLaMA-
HD-1.0. We perform stratified random sampling to
preserve the fraction of samples from each of the
datasets used in the experiment, maintaining the
importance of each dataset relative to the unfiltered
model.
SFT: Learning Rate: 1e-5, Maximum steps:
5000, Epochs: 1, Optimizer: Adamw, LR Sched-
uler: cosine, Maximum text length: 512, Batch
size: 4, Grad accumulation steps: 1, weight decay:
0.05. LoRA: Rank: 16, Alpha: 32, Dropout: 0.05.
Reward Model: Learning rate: 2e-5, Epochs: 1,
Optimizer: Adamw, LR Scheduler: linear, Maxi-
mum text length: 512, Batch size: 4, Gradient ac-
cumulation steps: 1, weight decay: 0.001. LoRA:
Rank: 8, Alpha: 32, Dropout: 0.1.
RLHF: Learning Rate: 1.41e-5, Maximum steps:
20000, Epochs: 4, Minimum generation length: 32,
Maximum generation length: 128, PPO Minibatch
size: 1, Batch size: 32, Gradient accumulation
steps: 4. LoRA: Rank: 16, Alpha: 32, Dropout:
0.05.
Quantitative Prompt
We list the prompt used for our qualitative exper-
iment here. The format is based on the multiple
choice one from PaLM (Chowdhery et al., 2022).
All quantitative metrics use this format. Details
about scoring are included in Section 3.4.
'{sentence} "{pronoun}" refers to: '
Qualitative Example
We report LLaMA-HD-0.2-S because it uses
the smallest fraction of OASST, yet consistently
demonstrates instruction following.
Prompt: 'What can I do in Miami,
FL in November?'
LLaMA-SFT:
'I'm
going
beach
summer...'
LLaMA-HD-0.2-S:
November,
enjoy
warm
weather...'