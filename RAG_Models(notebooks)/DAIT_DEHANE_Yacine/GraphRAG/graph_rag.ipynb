{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG\n",
    "\n",
    "<img src=\"./media/graphRAG_Architecture.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph_RAG Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import subprocess\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from smolagents import OpenAIServerModel, CodeAgent, ToolCallingAgent, HfApiModel, tool, GradioUI\n",
    "from datasets import load_metric\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === CONFIG ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = \"D:/2CSI-Project/PDFs_papers/*.pdf\"\n",
    "chunk_size = 1200\n",
    "overlap = 100\n",
    "output_file = \"graph_output.json\"\n",
    "tool_model_id = os.getenv(\"TOOL_MODEL_ID\")\n",
    "huggingface_api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "db_dir = r\"D:\\\\2CSI-Project\\\\VectorDB_Embeddings\"\n",
    "graph_root_path = \"./ragtest\"\n",
    "INPUT_PDF_DIR = r\"C:\\Users\\ACER NITRO\\OneDrive\\Bureau\\2CSI\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG_deepseek\\input\"\n",
    "OUTPUT_TXT_DIR = r\"C:\\Users\\ACER NITRO\\OneDrive\\Bureau\\2CSI\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG_deepseek\\ragtest\\input\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === MODEL SELECTOR ===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === PreProcess Scientific Papers 'PDF -> .txt' ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(OUTPUT_TXT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = unidecode(text)\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) < 4:\n",
    "            continue\n",
    "        if re.match(r\"^\\s*(Page)?\\s*\\d+\\s*$\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        if re.search(r\"(logo|www\\.|http|copyright|all rights reserved)\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
    "    cleaned_text = re.sub(r\"\\n{2,}\", \"\\n\\n\", cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def process_pdf(pdf_path: str, output_dir: str, split_chunks=False, chunk_size=1200, chunk_overlap=100):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    doc.close()\n",
    "\n",
    "    cleaned = clean_text(full_text)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_txt_path = os.path.join(output_dir, f\"{base_name}.txt\")\n",
    "\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned)\n",
    "\n",
    "    print(f\"âœ… Cleaned text saved to: {output_txt_path}\")\n",
    "\n",
    "    if split_chunks:\n",
    "        text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        chunks = text_splitter.split_text(cleaned)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_path = os.path.join(output_dir, f\"{base_name}_chunk_{idx}.txt\")\n",
    "            with open(chunk_path, \"w\", encoding=\"utf-8\") as cf:\n",
    "                cf.write(chunk)\n",
    "        print(f\"ðŸ§© {len(chunks)} chunks saved for: {base_name}\")\n",
    "\n",
    "def batch_process_pdfs(input_dir: str, output_dir: str, split_chunks=False):\n",
    "    pdf_files = glob(os.path.join(input_dir, \"*.pdf\"))\n",
    "    print(f\"ðŸ“š Found {len(pdf_files)} PDFs.\")\n",
    "\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        process_pdf(pdf_path, output_dir, split_chunks=split_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- PDF Processing and Chunking ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Found 0 PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_process_pdfs(INPUT_PDF_DIR, OUTPUT_TXT_DIR, split_chunks=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ==== Graph Visualization ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at Final Entities and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>graph_embedding</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>description_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b45241d70f0e43fca764df95b2b81f77</td>\n",
       "      <td>\"HERBERT WOISETSCHLAGER, ALEXANDER ERBEN, SHIQ...</td>\n",
       "      <td>\"PERSON\", \"ORGANIZATION\")(\"ENTITY\"</td>\n",
       "      <td>\"ACM Reference Format\"</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[e9aa2e30f8a16ab2a2fa9be1c46e9de1]</td>\n",
       "      <td>[-0.0522812083363533, -0.2714383900165558, -3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4119fd06010c494caa07f439b333f4c5</td>\n",
       "      <td>\"AGX ORIN\"</td>\n",
       "      <td>\"ORGANIZATION\", \"AGX ORIN\"</td>\n",
       "      <td>\"person\"</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>[0d66389e93327df8f525277dda4617e5]</td>\n",
       "      <td>[-0.195903941988945, 0.6456646919250488, -3.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d3835bf3dda84ead99deadbeac5d0d7d</td>\n",
       "      <td>\"ORGANIZATION\"</td>\n",
       "      <td>\"THE ORGANIZATION IS NOT EXPLICITLY MENTIONED ...</td>\n",
       "      <td>The Organization, a prominent entity, holds a...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>[7befbf2cdd18e8189b0f6e34637a77f3, 7c22470c632...</td>\n",
       "      <td>[-0.5362508296966553, 1.8357473611831665, -3.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>077d2820ae1845bcbb1803379a3d1eae</td>\n",
       "      <td>\"GNNS\"</td>\n",
       "      <td>\"TECHNOLOGY\"</td>\n",
       "      <td>\"GNNs\" refers to Graph Neural Networks, a type...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>[242307f545da2144b2e3affbd99017d2]</td>\n",
       "      <td>[0.17368170619010925, 2.1736154556274414, -3.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3671ea0dd4e84c1a9b02c5ab2c8f4bac</td>\n",
       "      <td>\"AI AND MACHINE LEARNING LANDSCAPE\"</td>\n",
       "      <td>\"CONCEPT\"</td>\n",
       "      <td>\"AI and Machine Learning Landscape\" is the fie...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[242307f545da2144b2e3affbd99017d2]</td>\n",
       "      <td>[-0.25016453862190247, 1.5578858852386475, -3....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  b45241d70f0e43fca764df95b2b81f77   \n",
       "1  4119fd06010c494caa07f439b333f4c5   \n",
       "2  d3835bf3dda84ead99deadbeac5d0d7d   \n",
       "3  077d2820ae1845bcbb1803379a3d1eae   \n",
       "4  3671ea0dd4e84c1a9b02c5ab2c8f4bac   \n",
       "\n",
       "                                                name  \\\n",
       "0  \"HERBERT WOISETSCHLAGER, ALEXANDER ERBEN, SHIQ...   \n",
       "1                                         \"AGX ORIN\"   \n",
       "2                                     \"ORGANIZATION\"   \n",
       "3                                             \"GNNS\"   \n",
       "4                \"AI AND MACHINE LEARNING LANDSCAPE\"   \n",
       "\n",
       "                                                type  \\\n",
       "0                 \"PERSON\", \"ORGANIZATION\")(\"ENTITY\"   \n",
       "1                         \"ORGANIZATION\", \"AGX ORIN\"   \n",
       "2  \"THE ORGANIZATION IS NOT EXPLICITLY MENTIONED ...   \n",
       "3                                       \"TECHNOLOGY\"   \n",
       "4                                          \"CONCEPT\"   \n",
       "\n",
       "                                         description  human_readable_id  \\\n",
       "0                             \"ACM Reference Format\"                  0   \n",
       "1                                           \"person\"                  1   \n",
       "2   The Organization, a prominent entity, holds a...                  2   \n",
       "3  \"GNNs\" refers to Graph Neural Networks, a type...                  3   \n",
       "4  \"AI and Machine Learning Landscape\" is the fie...                  4   \n",
       "\n",
       "  graph_embedding                                      text_unit_ids  \\\n",
       "0            None                 [e9aa2e30f8a16ab2a2fa9be1c46e9de1]   \n",
       "1            None                 [0d66389e93327df8f525277dda4617e5]   \n",
       "2            None  [7befbf2cdd18e8189b0f6e34637a77f3, 7c22470c632...   \n",
       "3            None                 [242307f545da2144b2e3affbd99017d2]   \n",
       "4            None                 [242307f545da2144b2e3affbd99017d2]   \n",
       "\n",
       "                               description_embedding  \n",
       "0  [-0.0522812083363533, -0.2714383900165558, -3....  \n",
       "1  [-0.195903941988945, 0.6456646919250488, -3.13...  \n",
       "2  [-0.5362508296966553, 1.8357473611831665, -3.1...  \n",
       "3  [0.17368170619010925, 2.1736154556274414, -3.5...  \n",
       "4  [-0.25016453862190247, 1.5578858852386475, -3....  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "entities = pd.read_parquet('./ragtest/output/20250421-001836/artifacts/create_final_entities.parquet')\n",
    "\n",
    "entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>source_degree</th>\n",
       "      <th>target_degree</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"GNNS\"</td>\n",
       "      <td>\"QUANTUM COMPUTING\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"GNNs will be enhanced by Quantum Computing, e...</td>\n",
       "      <td>[242307f545da2144b2e3affbd99017d2]</td>\n",
       "      <td>6fae5ee1a831468aa585a1ea09095998</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLASSIFICATION DIFFICULTY\"</td>\n",
       "      <td>\"CONCEPT\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"The complexity level of a feature determines ...</td>\n",
       "      <td>[6432a1a2eeff7c0f772b6fd06da0131a]</td>\n",
       "      <td>ef32c4b208d041cc856f6837915dc1b0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"IBAN, E-MAIL ADDRESS, POSTAL CODES\"</td>\n",
       "      <td>\"FEATURES\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"These features are examples of simple feature...</td>\n",
       "      <td>[6432a1a2eeff7c0f772b6fd06da0131a]</td>\n",
       "      <td>07b2425216bd4f0aa4e079827cb48ef5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"RULE-BASED (E.G., REGULAR EXPRESSIONS)\"</td>\n",
       "      <td>\"EXTRACTION METHODS\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Simple features can be extracted using rule-b...</td>\n",
       "      <td>[6432a1a2eeff7c0f772b6fd06da0131a]</td>\n",
       "      <td>2670deebfa3f4d69bb82c28ab250a209</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"NAMED ENTITIES (E.G., ORGANIZATION, PERSON NA...</td>\n",
       "      <td>\"FEATURES\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"These features are examples of dynamic featur...</td>\n",
       "      <td>[6432a1a2eeff7c0f772b6fd06da0131a]</td>\n",
       "      <td>404309e89a5241d6bff42c05a45df206</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source                target  \\\n",
       "0                                             \"GNNS\"   \"QUANTUM COMPUTING\"   \n",
       "1                        \"CLASSIFICATION DIFFICULTY\"             \"CONCEPT\"   \n",
       "2               \"IBAN, E-MAIL ADDRESS, POSTAL CODES\"            \"FEATURES\"   \n",
       "3           \"RULE-BASED (E.G., REGULAR EXPRESSIONS)\"  \"EXTRACTION METHODS\"   \n",
       "4  \"NAMED ENTITIES (E.G., ORGANIZATION, PERSON NA...            \"FEATURES\"   \n",
       "\n",
       "   weight                                        description  \\\n",
       "0     1.0  \"GNNs will be enhanced by Quantum Computing, e...   \n",
       "1     1.0  \"The complexity level of a feature determines ...   \n",
       "2     1.0  \"These features are examples of simple feature...   \n",
       "3     1.0  \"Simple features can be extracted using rule-b...   \n",
       "4     1.0  \"These features are examples of dynamic featur...   \n",
       "\n",
       "                        text_unit_ids                                id  \\\n",
       "0  [242307f545da2144b2e3affbd99017d2]  6fae5ee1a831468aa585a1ea09095998   \n",
       "1  [6432a1a2eeff7c0f772b6fd06da0131a]  ef32c4b208d041cc856f6837915dc1b0   \n",
       "2  [6432a1a2eeff7c0f772b6fd06da0131a]  07b2425216bd4f0aa4e079827cb48ef5   \n",
       "3  [6432a1a2eeff7c0f772b6fd06da0131a]  2670deebfa3f4d69bb82c28ab250a209   \n",
       "4  [6432a1a2eeff7c0f772b6fd06da0131a]  404309e89a5241d6bff42c05a45df206   \n",
       "\n",
       "  human_readable_id  source_degree  target_degree  rank  \n",
       "0                 0              1              1     2  \n",
       "1                 1              1              1     2  \n",
       "2                 2              1              2     3  \n",
       "3                 3              1              2     3  \n",
       "4                 4              1              2     3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships = pd.read_parquet('./ragtest/output/20250421-001836/artifacts/create_final_relationships.parquet')\n",
    "\n",
    "relationships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining all of this with our relationships gives us our final nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>source_id</th>\n",
       "      <th>degree</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>id</th>\n",
       "      <th>size</th>\n",
       "      <th>graph_embedding</th>\n",
       "      <th>community</th>\n",
       "      <th>top_level_node_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"HERBERT WOISETSCHLAGER, ALEXANDER ERBEN, SHIQ...</td>\n",
       "      <td>\"PERSON\", \"ORGANIZATION\")(\"ENTITY\"</td>\n",
       "      <td>\"ACM Reference Format\"</td>\n",
       "      <td>e9aa2e30f8a16ab2a2fa9be1c46e9de1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>b45241d70f0e43fca764df95b2b81f77</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b45241d70f0e43fca764df95b2b81f77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"AGX ORIN\"</td>\n",
       "      <td>\"ORGANIZATION\", \"AGX ORIN\"</td>\n",
       "      <td>\"person\"</td>\n",
       "      <td>0d66389e93327df8f525277dda4617e5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4119fd06010c494caa07f439b333f4c5</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4119fd06010c494caa07f439b333f4c5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\"ORGANIZATION\"</td>\n",
       "      <td>\"THE ORGANIZATION IS NOT EXPLICITLY MENTIONED ...</td>\n",
       "      <td>The Organization, a prominent entity, holds a...</td>\n",
       "      <td>7befbf2cdd18e8189b0f6e34637a77f3,7c22470c6324e...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>d3835bf3dda84ead99deadbeac5d0d7d</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>d3835bf3dda84ead99deadbeac5d0d7d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"GNNS\"</td>\n",
       "      <td>\"TECHNOLOGY\"</td>\n",
       "      <td>\"GNNs\" refers to Graph Neural Networks, a type...</td>\n",
       "      <td>242307f545da2144b2e3affbd99017d2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>077d2820ae1845bcbb1803379a3d1eae</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>077d2820ae1845bcbb1803379a3d1eae</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"AI AND MACHINE LEARNING LANDSCAPE\"</td>\n",
       "      <td>\"CONCEPT\"</td>\n",
       "      <td>\"AI and Machine Learning Landscape\" is the fie...</td>\n",
       "      <td>242307f545da2144b2e3affbd99017d2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3671ea0dd4e84c1a9b02c5ab2c8f4bac</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3671ea0dd4e84c1a9b02c5ab2c8f4bac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>\"QUANTUM COMPUTING\"</td>\n",
       "      <td>\"TECHNOLOGY\"</td>\n",
       "      <td>\"Quantum Computing\" is a technology that can e...</td>\n",
       "      <td>242307f545da2144b2e3affbd99017d2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>19a7f254a5d64566ab5cc15472df02de</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19a7f254a5d64566ab5cc15472df02de</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>\"ADVANCEMENTS\"</td>\n",
       "      <td>\"EVENT\"</td>\n",
       "      <td>\"Advancements\" refers to the continuous improv...</td>\n",
       "      <td>242307f545da2144b2e3affbd99017d2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>e7ffaee9d31d4d3c96e04f911d0a8f9e</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>e7ffaee9d31d4d3c96e04f911d0a8f9e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>\"DEEPSET\"</td>\n",
       "      <td>\"ORGANIZATION\", \"MOLLER ET AL.\"</td>\n",
       "      <td>The DEEPSET, established in 2023, is an annot...</td>\n",
       "      <td>6432a1a2eeff7c0f772b6fd06da0131a,af767269307bc...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>f7e11b0e297a44a896dc67928368f600</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>f7e11b0e297a44a896dc67928368f600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>\"SQUAD DATASET\"</td>\n",
       "      <td>\"EVENT\", \"RAJPURKAR ET AL.\"</td>\n",
       "      <td>\"person\", \"Cloud-era Fast Forward Labs\"</td>\n",
       "      <td>af767269307bcd4abab0dc93481d3a9c</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1fd3fa8bb5a2408790042ab9573779ee</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1fd3fa8bb5a2408790042ab9573779ee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>\"HUGGINGFACE COMMUNITY\"</td>\n",
       "      <td>\"ORGANIZATION\", \"WOLF ET AL.\"</td>\n",
       "      <td>\"person\", \"QA models\"</td>\n",
       "      <td>af767269307bcd4abab0dc93481d3a9c</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>27f9fbe6ad8c4a8b9acee0d3596ed57c</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27f9fbe6ad8c4a8b9acee0d3596ed57c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level                                              title  \\\n",
       "0      0  \"HERBERT WOISETSCHLAGER, ALEXANDER ERBEN, SHIQ...   \n",
       "1      0                                         \"AGX ORIN\"   \n",
       "2      0                                     \"ORGANIZATION\"   \n",
       "3      0                                             \"GNNS\"   \n",
       "4      0                \"AI AND MACHINE LEARNING LANDSCAPE\"   \n",
       "5      0                                \"QUANTUM COMPUTING\"   \n",
       "6      0                                     \"ADVANCEMENTS\"   \n",
       "7      0                                          \"DEEPSET\"   \n",
       "8      0                                    \"SQUAD DATASET\"   \n",
       "9      0                            \"HUGGINGFACE COMMUNITY\"   \n",
       "\n",
       "                                                type  \\\n",
       "0                 \"PERSON\", \"ORGANIZATION\")(\"ENTITY\"   \n",
       "1                         \"ORGANIZATION\", \"AGX ORIN\"   \n",
       "2  \"THE ORGANIZATION IS NOT EXPLICITLY MENTIONED ...   \n",
       "3                                       \"TECHNOLOGY\"   \n",
       "4                                          \"CONCEPT\"   \n",
       "5                                       \"TECHNOLOGY\"   \n",
       "6                                            \"EVENT\"   \n",
       "7                    \"ORGANIZATION\", \"MOLLER ET AL.\"   \n",
       "8                        \"EVENT\", \"RAJPURKAR ET AL.\"   \n",
       "9                      \"ORGANIZATION\", \"WOLF ET AL.\"   \n",
       "\n",
       "                                         description  \\\n",
       "0                             \"ACM Reference Format\"   \n",
       "1                                           \"person\"   \n",
       "2   The Organization, a prominent entity, holds a...   \n",
       "3  \"GNNs\" refers to Graph Neural Networks, a type...   \n",
       "4  \"AI and Machine Learning Landscape\" is the fie...   \n",
       "5  \"Quantum Computing\" is a technology that can e...   \n",
       "6  \"Advancements\" refers to the continuous improv...   \n",
       "7   The DEEPSET, established in 2023, is an annot...   \n",
       "8            \"person\", \"Cloud-era Fast Forward Labs\"   \n",
       "9                              \"person\", \"QA models\"   \n",
       "\n",
       "                                           source_id  degree  \\\n",
       "0                   e9aa2e30f8a16ab2a2fa9be1c46e9de1       0   \n",
       "1                   0d66389e93327df8f525277dda4617e5       0   \n",
       "2  7befbf2cdd18e8189b0f6e34637a77f3,7c22470c6324e...       0   \n",
       "3                   242307f545da2144b2e3affbd99017d2       1   \n",
       "4                   242307f545da2144b2e3affbd99017d2       0   \n",
       "5                   242307f545da2144b2e3affbd99017d2       1   \n",
       "6                   242307f545da2144b2e3affbd99017d2       0   \n",
       "7  6432a1a2eeff7c0f772b6fd06da0131a,af767269307bc...       0   \n",
       "8                   af767269307bcd4abab0dc93481d3a9c       0   \n",
       "9                   af767269307bcd4abab0dc93481d3a9c       0   \n",
       "\n",
       "   human_readable_id                                id  size graph_embedding  \\\n",
       "0                  0  b45241d70f0e43fca764df95b2b81f77     0            None   \n",
       "1                  1  4119fd06010c494caa07f439b333f4c5     0            None   \n",
       "2                  2  d3835bf3dda84ead99deadbeac5d0d7d     0            None   \n",
       "3                  3  077d2820ae1845bcbb1803379a3d1eae     1            None   \n",
       "4                  4  3671ea0dd4e84c1a9b02c5ab2c8f4bac     0            None   \n",
       "5                  5  19a7f254a5d64566ab5cc15472df02de     1            None   \n",
       "6                  6  e7ffaee9d31d4d3c96e04f911d0a8f9e     0            None   \n",
       "7                  7  f7e11b0e297a44a896dc67928368f600     0            None   \n",
       "8                  8  1fd3fa8bb5a2408790042ab9573779ee     0            None   \n",
       "9                  9  27f9fbe6ad8c4a8b9acee0d3596ed57c     0            None   \n",
       "\n",
       "  community                 top_level_node_id  x  y  \n",
       "0      None  b45241d70f0e43fca764df95b2b81f77  0  0  \n",
       "1      None  4119fd06010c494caa07f439b333f4c5  0  0  \n",
       "2      None  d3835bf3dda84ead99deadbeac5d0d7d  0  0  \n",
       "3      None  077d2820ae1845bcbb1803379a3d1eae  0  0  \n",
       "4      None  3671ea0dd4e84c1a9b02c5ab2c8f4bac  0  0  \n",
       "5      None  19a7f254a5d64566ab5cc15472df02de  0  0  \n",
       "6      None  e7ffaee9d31d4d3c96e04f911d0a8f9e  0  0  \n",
       "7      None  f7e11b0e297a44a896dc67928368f600  0  0  \n",
       "8      None  1fd3fa8bb5a2408790042ab9573779ee  0  0  \n",
       "9      None  27f9fbe6ad8c4a8b9acee0d3596ed57c  0  0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = pd.read_parquet('./ragtest/output/20250421-001836/artifacts/create_final_nodes.parquet')\n",
    "\n",
    "nodes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Community Report Generation & Summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community</th>\n",
       "      <th>full_content</th>\n",
       "      <th>level</th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>rank_explanation</th>\n",
       "      <th>summary</th>\n",
       "      <th>findings</th>\n",
       "      <th>full_content_json</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td># NLP Model Accuracy Comparison\\n\\nThis report...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>NLP Model Accuracy Comparison</td>\n",
       "      <td>The high impact severity rating is due to the ...</td>\n",
       "      <td>This report presents a comparison of four NLP ...</td>\n",
       "      <td>[{'explanation': 'The accuracy of BERT-FT was ...</td>\n",
       "      <td>{\\n    \"title\": \"NLP Model Accuracy Comparison...</td>\n",
       "      <td>ae255080-895f-40dc-86db-e8ad6c7f4d52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td># NLP Model Training Datasets Community\\n\\nThi...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NLP Model Training Datasets Community</td>\n",
       "      <td>The impact severity rating of 7.5 indicates th...</td>\n",
       "      <td>This report examines a community centered arou...</td>\n",
       "      <td>[{'explanation': 'The DGA dataset, with a degr...</td>\n",
       "      <td>{\\n    \"title\": \"NLP Model Training Datasets C...</td>\n",
       "      <td>9e9bff7b-304b-4852-bbb6-a840474f8384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  community                                       full_content  level  rank  \\\n",
       "0         1  # NLP Model Accuracy Comparison\\n\\nThis report...      0   9.7   \n",
       "1         3  # NLP Model Training Datasets Community\\n\\nThi...      0   7.5   \n",
       "\n",
       "                                   title  \\\n",
       "0          NLP Model Accuracy Comparison   \n",
       "1  NLP Model Training Datasets Community   \n",
       "\n",
       "                                    rank_explanation  \\\n",
       "0  The high impact severity rating is due to the ...   \n",
       "1  The impact severity rating of 7.5 indicates th...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  This report presents a comparison of four NLP ...   \n",
       "1  This report examines a community centered arou...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  [{'explanation': 'The accuracy of BERT-FT was ...   \n",
       "1  [{'explanation': 'The DGA dataset, with a degr...   \n",
       "\n",
       "                                   full_content_json  \\\n",
       "0  {\\n    \"title\": \"NLP Model Accuracy Comparison...   \n",
       "1  {\\n    \"title\": \"NLP Model Training Datasets C...   \n",
       "\n",
       "                                     id  \n",
       "0  ae255080-895f-40dc-86db-e8ad6c7f4d52  \n",
       "1  9e9bff7b-304b-4852-bbb6-a840474f8384  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_reports = pd.read_parquet('./ragtest/output/20250421-001836/artifacts/create_final_community_reports.parquet')\n",
    "\n",
    "community_reports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Graph RAG Retrieval Function ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_graphrag(\n",
    "    query: str,\n",
    "    method: str = \"global\",\n",
    "    root_path: str = \"./ragtest\",\n",
    "    timeout: Optional[int] = None,\n",
    "    community_level: int = 2,\n",
    "    dynamic_community_selection: bool = False\n",
    ") -> str:\n",
    "    if community_level < 0:\n",
    "        raise ValueError(\"Community level must be non-negative\")\n",
    "\n",
    "    command = [\n",
    "        \"python\", \"-m\", \"graphrag.query\",\n",
    "        \"--root\", root_path,\n",
    "        \"--method\", method,\n",
    "        \"--community_level\", str(community_level)\n",
    "    ]\n",
    "\n",
    "    if dynamic_community_selection:\n",
    "        command.append(\"--dynamic-community-selection\")\n",
    "\n",
    "    command.append(query)\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        result.check_returncode()\n",
    "        return result.stdout.strip()\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"=== STDOUT ===\")\n",
    "        print(e.stdout)\n",
    "        print(\"=== STDERR ===\")\n",
    "        print(e.stderr)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Search\n",
    "\n",
    "<img src=\"./media/local_search.png\" width=900>\n",
    "\n",
    "The GraphRAG approach to local search is the most similar to regular semantic RAG search. It combines structured data from the knowledge graph with unstructured data from the input documents to augment the LLM context with relevant entity information. In essence, we are going to first search for relevant entities to the query using semantic search. These become the entry points on our graph that we can now traverse. Starting at these points, we look at connected chunks of text, community reports, other entities, and relationships between them. All of the data retrieved is filtered and ranked to fit into a pre-defined context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STDOUT ===\n",
      "\n",
      "\n",
      "INFO: Reading settings from C:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\ragtest\\settings.yaml\n",
      "creating llm client with {'api_key': 'REDACTED,len=9', 'type': \"openai_chat\", 'model': 'deepseek-r1:1.5b', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'request_timeout': 180.0, 'api_base': 'http://localhost:11434/v1', 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
      "creating embedding llm client with {'api_key': 'REDACTED,len=9', 'type': \"openai_embedding\", 'model': 'all-minilm:l6-v2', 'max_tokens': 4000, 'temperature': 0, 'top_p': 1, 'request_timeout': 180.0, 'api_base': 'http://localhost:11434/api', 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': None, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
      "Error embedding chunk {'OpenAIEmbedding': \"'NoneType' object is not iterable\"}\n",
      "\n",
      "=== STDERR ===\n",
      "2025-04-21 22:48:57.340837: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-21 22:48:59.497300: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\query\\__main__.py\", line 76, in <module>\n",
      "    run_local_search(\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\query\\cli.py\", line 154, in run_local_search\n",
      "    result = search_engine.search(query=query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\query\\structured_search\\local_search\\search.py\", line 118, in search\n",
      "    context_text, context_records = self.context_builder.build_context(\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\query\\structured_search\\local_search\\mixed_context.py\", line 139, in build_context\n",
      "    selected_entities = map_query_to_entities(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\query\\context_builder\\entity_extraction.py\", line 55, in map_query_to_entities\n",
      "    search_results = text_embedding_vectorstore.similarity_search_by_text(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\vector_stores\\lancedb.py\", line 118, in similarity_search_by_text\n",
      "    query_embedding = text_embedder(text)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\query\\context_builder\\entity_extraction.py\", line 57, in <lambda>\n",
      "    text_embedder=lambda t: text_embedder.embed(t),\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graphrag\\query\\llm\\oai\\embedding.py\", line 96, in embed\n",
      "    chunk_embeddings = np.average(chunk_embeddings, axis=0, weights=chunk_lens)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anaconda3\\envs\\SEDS_ML\\Lib\\site-packages\\numpy\\lib\\function_base.py\", line 550, in average\n",
      "    raise ZeroDivisionError(\n",
      "ZeroDivisionError: Weights sum to zero, can't be normalized\n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', '-m', 'graphrag.query', '--root', 'C:\\\\Users\\\\ACER NITRO\\\\OneDrive\\\\Bureau\\\\Project 2SCI\\\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\\\RAG_Models(notebooks)\\\\DAIT_DEHANE_Yacine\\\\GraphRAG\\\\ragtest', '--method', 'local', 'What is machine learning?']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graph_rag.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m query_graphrag(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     query\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhat is machine learning?\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlocal\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     root_path\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mACER NITRO\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mOneDrive\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mBureau\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mProject 2SCI\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mAutomated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mRAG_Models(notebooks)\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDAIT_DEHANE_Yacine\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGraphRAG\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mragtest\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQuery result:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[1;32mc:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\graph_rag.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     result \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mrun(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         command,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         capture_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         text\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     result\u001b[39m.\u001b[39;49mcheck_returncode()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER%20NITRO/OneDrive/Bureau/Project%202SCI/Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles/RAG_Models%28notebooks%29/DAIT_DEHANE_Yacine/GraphRAG/graph_rag.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\anaconda3\\envs\\SEDS_ML\\Lib\\subprocess.py:502\u001b[0m, in \u001b[0;36mCompletedProcess.check_returncode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Raise CalledProcessError if the exit code is non-zero.\"\"\"\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode:\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout,\n\u001b[0;32m    503\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['python', '-m', 'graphrag.query', '--root', 'C:\\\\Users\\\\ACER NITRO\\\\OneDrive\\\\Bureau\\\\Project 2SCI\\\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\\\RAG_Models(notebooks)\\\\DAIT_DEHANE_Yacine\\\\GraphRAG\\\\ragtest', '--method', 'local', 'What is machine learning?']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"What is machine learning?\",\n",
    "    method=\"local\",\n",
    "    root_path=r\"C:\\Users\\ACER NITRO\\OneDrive\\Bureau\\Project 2SCI\\Automated-Information-Retrieval-and-Summarization-for-Academic-Research-Articles\\RAG_Models(notebooks)\\DAIT_DEHANE_Yacine\\GraphRAG\\ragtest\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Search\n",
    "\n",
    "<img src=\"./media/global_search.png\" width=1000>\n",
    "\n",
    "Through the semantic clustering of communities during the indexxing process outlined above we created community reports as summaries of high level themes across these groupings. Having this community summary data at various levels allows us to do something that traditional RAG performs poorly at, answering queries about broad themes and ideas across our unstructured data.\n",
    "\n",
    "To capture as much broad information as possible in an efficient manner, GraphRAG implements a [map reduce](https://en.wikipedia.org/wiki/MapReduce) approach. Given a query, relevant community node reports at a specific hierarchical level are retrieved. These are shuffled and chunked, where each chunk is used to generate a list of points that each have their own \"importance score\". These intermediate points are ranked and filtered, attempting to maintain the most important points. These become the aggregate intermediary response, which is passed to the LLM as the context for the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n",
      "creating llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_chat\", 'encoding_model': 'cl100k_base', 'model': 'gpt-4o', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "\n",
      "SUCCESS: Global Search Response:\n",
      "### Choosing Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company is deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) approaches, several key factors must be considered. These factors include the specific requirements of the application, the need for external data integration, computational resource constraints, and the desired level of model adaptation and performance.\n",
      "\n",
      "#### Application Requirements\n",
      "\n",
      "The choice largely depends on the specific needs of the application. For instance, if the application requires high precision and adaptability, such as in customer support or information retrieval systems, RAG may be the preferred choice. RAG enhances large language models by integrating external data, allowing for more accurate and contextually relevant content generation without extensive fine-tuning [Data: Reports (19, 20, 35, 21, 36, 38, 60)].\n",
      "\n",
      "#### Computational Resources\n",
      "\n",
      "Computational resources are another critical consideration. Fine-tuning involves further training pre-trained models to improve their performance on specific tasks by updating the model's parameters using a smaller, domain-specific dataset. This process can be resource-intensive but is essential for adapting models to targeted applications and domains [Data: Reports (19, 45, 31, 60)].\n",
      "\n",
      "In contrast, PEFT approaches, such as LoRA and QLoRA, are designed to enhance memory efficiency and reduce computational costs during the fine-tuning process. These techniques allow for fine-tuning on less powerful hardware while maintaining performance levels comparable to traditional methods. PEFT is particularly useful for fine-tuning multimodal models and deploying NLP models on mobile devices [Data: Reports (36, 38, 19, 35)].\n",
      "\n",
      "#### Model Customization and Contextual Relevance\n",
      "\n",
      "The desired level of model customization and contextual relevance also plays a significant role. Fine-tuning is crucial for enhancing the performance of large language models (LLMs) on specific tasks, addressing challenges such as scalability, memory requirements, and resource efficiency. It allows for the customization of models to meet specialized needs and ensures they perform optimally in their intended environments [Data: Reports (45, 31, 60)].\n",
      "\n",
      "RAG systems, on the other hand, improve the accuracy and contextuality of the outputs produced by LLMs, making them suitable for applications where context is crucial [Data: Reports (20, 21, 53)].\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Ultimately, the decision between RAG, fine-tuning, and PEFT approaches should be guided by the specific needs and constraints of the company, such as the importance of context relevance, the need for task-specific optimization, and resource availability. By carefully evaluating these factors, companies can select the most appropriate method to achieve their desired outcomes [Data: Reports (19, 20, 35, 21, 36, 38, 60, 53, 45)].\n"
     ]
    }
   ],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"global\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRIFT Search\n",
    "\n",
    "<img src=\"./media/drift_search.png\" width=1000>\n",
    "\n",
    "[Dynamic Reasoning and Inference with Flexible Traversal](https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/), or DRIFT, is a novel GraphRAG concept introduced by Microsoft as an approach to local search queries that include community information in the search process.\n",
    "\n",
    "The user's query is initially processed through [Hypothetical Document Embedding (HyDE)](https://arxiv.org/pdf/2212.10496), which creates a hypothetical document similar to those found in the graph already, but using the user's topic query. This document is embedded and used for semantic retrieval of the top-k relevant community reports. From these matches, we generate an initial answer along with several follow-up questions as a lightweight version of global search. They refer to this as the primer.\n",
    "\n",
    "Once this primer phase is complete, we execute local searches for each follow-up question generated. Each local search produces both intermediate answers and new follow-up questions, creating a refinement loop. This loop runs for two iterations (noted future research planned to develop reward functions for smarter termination). An important note that makes these local searches unique is that they are informed by both community-level knowledge and detailed entity/relationship data. This allows the DRIFT process to find relevant information even when the initial query diverges from the indexing persona, and it can adapt its approach based on emerging information during the search.\n",
    "\n",
    "The final output is structured as a hierarchy of questions and answers, ranked by their relevance to the original query. Map reduce is used again with an equal weighting on all intermediate answers, then passed to the language model for a final response. DRIFT cleverly combines global and local search with guided exploration to provide both broad context and specific details in responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n",
      "INFO: Vector Store Args: {\n",
      "    \"type\": \"lancedb\",\n",
      "    \"db_uri\": \"/Users/adamlucek/Desktop/github/GraphRAG/ragtest/output/lancedb\",\n",
      "    \"container_name\": \"==== REDACTED ====\",\n",
      "    \"overwrite\": true\n",
      "}\n",
      "creating llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_chat\", 'encoding_model': 'cl100k_base', 'model': 'gpt-4o', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "creating embedding llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_embedding\", 'encoding_model': 'cl100k_base', 'model': 'text-embedding-3-small', 'max_tokens': 4000, 'temperature': 0, 'top_p': 1, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': None, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "\n",
      "SUCCESS: DRIFT Search Response:\n",
      "# Understanding the Choice Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company is faced with the decision to choose between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches, several factors must be taken into account, determined by the specific application requirements and resource availability.\n",
      "\n",
      "## Retrieving Capabilities with RAG\n",
      "RAG is particularly advantageous in scenarios where it is crucial to integrate external information into language models for enhanced accuracy and relevance. It is highly recommended for applications like question and answer systems, customer support, and summarization tasks, where real-time context retrieval from large datasets improves response precision without needing extensive and expensive model retraining. The strength of RAG lies in its ability to quickly adapt to various knowledge domains by leveraging existing data without modifying the underlying language model significantly.\n",
      "\n",
      "## The Focus and Depth of Fine-Tuning\n",
      "Traditional fine-tuning is the go-to choice when a clear, defined improvement in task performance is needed using domain-specific datasets. It involves re-calibrating a pre-trained modelâ€™s parameters to cater precisely to industry-specific or task-specific requirements. This approach can enhance performance metrics significantly but incurs high computational costs and demands substantial amounts of fine-tuning data, making it ideal for well-funded projects aiming for precise task optimization.\n",
      "\n",
      "## Resource-Efficiency with PEFT Techniques\n",
      "On the other end of the spectrum, PEFT approaches such as DORA and LoRA present themselves as highly resource-efficient alternatives to standard fine-tuning. They are suitable for companies with limited computational resources but still prioritize performance enhancements. By selectively adjusting additional smaller model parameters, PEFT methods allow substantial flexibility and significant model adaptability for various applications without the full overhead of conventional fine-tuning.\n",
      "\n",
      "Ultimately, the decision for a company involves balancing resource availability (both in terms of data and computing power), desired performance efficiency, and the specific features of the task at hand. Companies with broad application needs across different domains might favor RAG's flexibility; domain-specific improvements would benefit more from deep fine-tuning, while constrained environments should consider PEFT methods for their adaptability and efficiency.\n",
      "\n",
      "# Understanding the Choice Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When choosing between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) approaches, businesses must evaluate their specific needs in terms of model performance, resource constraints, and flexibility. Each approach involves distinct methods for optimally leveraging machine learning models and NLP systems.\n",
      "\n",
      "**Retrieval-Augmented Generation (RAG):** RAG is a strategy combining retrieval and generative models to enhance the context provided to language models like GPT-3. This approach allows models to answer queries more accurately by accessing external knowledge bases. It is particularly beneficial when dealing with tasks requiring extensive context or up-to-date information. Companies would favor RAG when the model needs to maintain relevance in dynamic information environments or when the contextual accuracy is crucial.\n",
      "\n",
      "**Fine-Tuning:** This traditional method involves adjusting the entire model's parameters to optimize performance on a specific task. Fine-tuning is often resource-intensive, requiring significant computational power and time, which can prove a bottleneck for large models or organizations with limited resources. It produces highly task-specific outcomes, making it an ideal choice when high accuracy is paramount and resources are abundant. Fine-tuning is suitable for specialized tasks where the nuances of data need careful embedding into the model.\n",
      "\n",
      "**Parameter-Efficient Fine-Tuning (PEFT):** Techniques like QLoRA and LoRA are designed to address the resource constraints associated with fine-tuning. By adjusting only a subset of parameters, these methods provide a more efficient approach, maintaining model performance while requiring less computational power. Companies might choose PEFT when deploying large models on resource-constrained devices, like mobile platforms, or when exploring multiple tasks with shared resources. PEFT's importance is growing in real-world applications where cost and efficiency are central concerns.\n",
      "\n",
      "In summary, choosing between these approaches depends on an organizationâ€™s priorities: RAG offers dynamic and contextual adaptability; fine-tuning promises high precision at the cost of resources; and PEFT balances resource demands with performance, making it suitable for scalable deployment. Organizations are encouraged to deeply understand their specific contextual and technical requirements to select the most appropriate approach.\n",
      "\n",
      "## How Companies Choose Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) techniques, companies must consider a variety of factors related to their unique needs and resources. Each approach serves distinct purposes and offers specific advantages, making them suitable for different scenarios.\n",
      "\n",
      "### Retrieval-Augmented Generation (RAG)\n",
      "RAG is particularly beneficial when a company requires a model to access a vast external data set to create highly contextual and informed responses. This method is preferred when the task involves processing or retrieving large amounts of dynamic information. The power of RAG lies in its ability to provide more comprehensive answers by leveraging external data sources, enhancing the model's output quality. As such, organizations where information retrieval and integration are crucial components of the application, such as in customer support systems or knowledge-based industries, may find RAG an optimal choice.\n",
      "\n",
      "### Fine-Tuning\n",
      "Fine-tuning a large language model involves adjusting its parameters based on specific tasks or domain data to improve its performance on said tasks. This method is ideal for companies aiming to develop a bespoke solution tailored to precise needs or niche applications. Fine-tuning a model can significantly increase its utility in specialized fields such as legal research or medical analysis, where the accuracy and specificity of information are critical. Though resource-intensive, the specificity in outcome makes traditional fine-tuning valuable for businesses that require high degrees of customization.\n",
      "\n",
      "### Parameter-Efficient Fine-Tuning (PEFT)\n",
      "PEFT techniques, such as LoRA and adapter-based methods, offer a way to enhance large language models without the typical computational cost associated with traditional fine-tuning. This strategy updates only a small subset of a model's parameters, thus making it a cost-effective alternative for companies with limited resources or those that need to frequently update models with new data. PEFT is advantageous for businesses that need flexibility without a significant investment in computational power, popular among startups or organizations exploring new avenues without extensive AI infrastructure.\n",
      "\n",
      "In summary, a company's choice between RAG, fine-tuning, and PEFT depends on their specific requirements, the nature and volume of data they interact with, and the computational resources at their disposal. By aligning their strategy with the strengths of each approach, businesses can optimize their use of AI technologies to meet their unique operational goals.\n",
      "\n",
      "# Evaluating Company Choices Among RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company considers enhancing its machine learning models, particularly large language models (LLMs), it encounters a range of techniques, including Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) methods like Half Fine-Tuning (HFT). Each of these techniques offers unique benefits and is suitable for different objectives and scenarios. Understanding the distinction and potential applications is crucial for making an informed decision.\n",
      "\n",
      "RAG is highly effective in situations where models need to provide accurate responses based on external information not inherently within the model's parameters. By integrating information retrieval techniques, RAG allows models to access a database or document store to augment their generated text, making it ideal for applications where dynamic content access and up-to-date information are critical, such as live customer service or research queries.\n",
      "\n",
      "In contrast, fine-tuning broadly involves adjusting an already pre-trained model to suit new, specific tasks or datasets. Traditional supervised fine-tuning is useful when the primary goal is to improve a modelâ€™s performance on specific tasks while leveraging existing knowledge. For example, in supervised fine-tuning, Half Fine-Tuning (HFT) offers an innovative balance by freezing a portion of the model's parameters, thus maintaining foundational knowledge while developing new competencies. This efficiency is especially important when handling models like the LLAMA 2-7B, where sustaining knowledge over several iterations is vital.\n",
      "\n",
      "PEFT techniques, such as HFT, provide a focused approach to fine-tuning by optimizing resource usage and reducing computational demands. They ensure that changes to the model preserve core functionalities while adapting to new requirements, which is critically important in environments where computational resources and model interpretability are constrained.\n",
      "\n",
      "The choice among RAG, fine-tuning, and PEFT approaches depends on factors such as the company's resource availability, existing model performance, specific use-cases, and the need for adaptability versus resource conservation. Companies prioritizing task-specific improvements with limited computational power might lean toward PEFT or HFT, whereas RAG would be preferable for scenarios needing comprehensive and current information access capabilities.\n",
      "\n",
      "Ultimately, the decision must align with strategic objectives, model lifecycle considerations, and the specific complexities of the data and questions the model is expected to handle.\n",
      "\n",
      "# Factors in Choosing Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company is deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) methods for large language models (LLMs), several important factors must be considered. These methods each have unique advantages and challenges that make them suitable for different scenarios.\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) enhances the performance of LLMs by integrating retrieval mechanisms that bring relevant information into the context before generating results. This approach is particularly useful when the task requires the contextually relevant retrieval of information from extensive databases or when the generation task is highly dependent on external knowledge. RAG systems improve both the accuracy and the contextual relevance of LLM outputs, making them valuable for applications where precision in response to contextual queries is critical.\n",
      "\n",
      "Fine-tuning involves adjusting the LLM using a carefully prepared dataset tailored to a specific task. This process includes essential stages such as dataset preparation, instruction tuning, and topic mapping. Fine-tuning is especially advantageous when the intention is to specialize a model for a specific application, ensuring nuanced comprehension and performance improvement for the target task. It provides more control over the model's behavior, as specific datasets align the model with intended outcomes, offering enhanced reliability.\n",
      "\n",
      "Parameter-Efficient Fine-Tuning (PEFT) approaches are designed to fine-tune LLMs with fewer resource requirements compared to traditional fine-tuning. Methods like LoRA (Low-Rank Adaptation), Adapter modules, and prefix tuning are examples of PEFT techniques. They focus on adapting a small set of parameters while maintaining the majority of the LLM fixed, thus reducing computational costs and the necessary dataset size. PEFT is particularly beneficial for scenarios with constrained computational resources or when rapidly deploying updates or adaptations is necessary without retraining the entire model.\n",
      "\n",
      "Ultimately, the choice between these approaches depends on the specific requirements and constraints of the task at hand, including the need for real-time retrieval capabilities (favoring RAG), the specificity and quality of output (favoring fine-tuning), or capacity and resource efficiency (favoring PEFT).\n"
     ]
    }
   ],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"drift\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing to Regular Vector Database Retrieval\n",
    "\n",
    "<img src=\"./media/basic_retrieval.png\" width=600>\n",
    " \n",
    "To give some comparison, let's look back at traditional chunking, embedding, and similarity retrieval RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate our Database**\n",
    "\n",
    "For this we'll be using [ChromaDB](https://www.trychroma.com) with the same chunks as were loaded into our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./notebook/chromadb\")\n",
    "paper_collection = chroma_client.get_or_create_collection(name=\"paper_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embed Chunks Into Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for text in texts:\n",
    "    paper_collection.add(\n",
    "        documents=[text],\n",
    "        ids=f\"chunk_{i}\"\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_retrieval(query, num_results=5):\n",
    "    results = paper_collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=num_results\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Prompt & Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = \"\"\"\n",
    "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
    "\n",
    "If you don't know the answer, just say so. Do not make anything up.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "Context: {retrieved_docs}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_rag(query):\n",
    "    retrieved_docs = chroma_retrieval(query)[\"documents\"][0]\n",
    "    response = rag_chain.invoke({\"retrieved_docs\": retrieved_docs, \"query\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When choosing between Retrieval-Augmented Generation (RAG), fine-tuning, and different Parameter-Efficient Fine-Tuning (PEFT) approaches, a company should consider several factors:\n",
      "\n",
      "1. **Data Access and Updates**: RAG is preferable for applications requiring access to external data sources or environments where data frequently updates. It provides dynamic data retrieval capabilities and is less prone to generating incorrect information.\n",
      "\n",
      "2. **Model Behavior and Domain-Specific Knowledge**: Fine-tuning is suitable when the model needs to adjust its behavior, writing style, or incorporate domain-specific knowledge. It is effective if there is ample domain-specific, labeled training data available.\n",
      "\n",
      "3. **Resource Constraints and Efficiency**: PEFT approaches like LoRA and DEFT are designed to reduce computational and resource requirements. LoRA focuses on low-rank matrices to reduce memory usage and computational load, while DEFT optimizes the fine-tuning process by focusing on the most critical data samples.\n",
      "\n",
      "4. **Task-Specific Adaptation**: If the goal is to adapt a model for specific tasks with minimal data, PEFT methods like DEFT and adapter-based techniques can be beneficial. They allow for efficient fine-tuning with fewer resources.\n",
      "\n",
      "5. **Transparency and Interpretability**: RAG systems offer more transparency and interpretability in the modelâ€™s decision-making process compared to solely fine-tuned models.\n",
      "\n",
      "6. **Scalability and Deployment**: For large-scale deployments, PEFT methods can significantly reduce computational costs by focusing on influential data samples and using surrogate models.\n",
      "\n",
      "In summary, the choice depends on the specific needs of the application, such as data access, model behavior, resource availability, and the importance of transparency and scalability.\n"
     ]
    }
   ],
   "source": [
    "response = chroma_rag(\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "**Traditional/Naive RAG:**\n",
    "\n",
    "Benefits:\n",
    "- Simpler implementation and deployment\n",
    "- Works well for straightforward information retrieval tasks\n",
    "- Good at handling unstructured text data\n",
    "- Lower computational overhead\n",
    "\n",
    "Drawbacks:\n",
    "- Loses structural information when chunking documents\n",
    "- Can break up related content during text segmentation\n",
    "- Limited ability to capture relationships between different pieces of information\n",
    "- May struggle with complex reasoning tasks requiring connecting multiple facts\n",
    "- Potential for incomplete or fragmented answers due to chunking boundaries\n",
    "\n",
    "**GraphRAG:**\n",
    "\n",
    "Benefits:\n",
    "- Preserves structural relationships and hierarchies in the knowledge\n",
    "- Better at capturing connections between related information\n",
    "- Can provide more complete and contextual answers\n",
    "- Improved retrieval accuracy by leveraging graph structure\n",
    "- Better supports complex reasoning across multiple facts\n",
    "- Can maintain document coherence better than chunk-based approaches\n",
    "- More interpretable due to explicit knowledge representation\n",
    "\n",
    "Drawbacks:\n",
    "- More complex to implement and maintain\n",
    "- Requires additional processing to construct and update knowledge graphs\n",
    "- Higher computational overhead for graph operations\n",
    "- May require domain expertise to define graph schema/structure\n",
    "- More challenging to scale to very large datasets\n",
    "- Additional storage requirements for graph structure\n",
    "\n",
    "**Key Differentiators:**\n",
    "1. Knowledge Representation: Traditional RAG treats everything as flat text chunks, while GraphRAG maintains structured relationships in a graph format\n",
    "\n",
    "2. Context Preservation: GraphRAG better preserves context and relationships between different pieces of information compared to the chunking approach of traditional RAG\n",
    "\n",
    "3. Reasoning Capability: GraphRAG enables better multi-hop reasoning and connection of related facts through graph traversal, while traditional RAG is more limited to direct retrieval\n",
    "\n",
    "4. Answer Quality: GraphRAG tends to produce more complete and coherent answers since it can access related information through graph connections rather than being limited by chunk boundaries\n",
    "\n",
    "The choice between traditional RAG and GraphRAG often depends on the specific use case, with GraphRAG being particularly valuable when maintaining relationships between information is important or when complex reasoning is required. An important note as well, GraphRAG approaches still rely on regular embedding and retrieval methods themselves. They compliment eahcother!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEDS_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
